---
layout: post
title: HipoRankæŠ½å–å¼æ— ç›‘ç£é•¿æ–‡æœ¬æ‘˜è¦
categories:
  - nlp
tags: []
date: 2022-06-28 11:12:00
---
é¡¹ç›®åœ°å€ï¼šhttps://github.com/mirandrom/HipoRank



æœ¬æ–‡ä½¿ç”¨ä¸€ä¸ªç®€å•çš„åŸºäºå›¾çš„ç®—æ³•ï¼Œå°†æ–‡æœ¬åˆ†ä¸ºä¸¤ä¸ªå±‚æ¬¡ï¼Œåˆ†åˆ«æ˜¯sectionå’Œsentenceï¼Œä»¥å‘ç°å¥å­é—´çš„ä¸°å¯Œçš„ç»“æ„ã€‚æœ¬æ–¹æ³•æ²¡æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹ï¼Œåªä½¿ç”¨äº†ä¸€ç§é‡è¦æ€§æ’åºçš„ç®—æ³•æ¥è®¡ç®—ã€‚æ•°æ®é›†ä¸­çš„train.txtä¹Ÿå¹¶æ²¡æœ‰ä½¿ç”¨åˆ°ã€‚

![upload successful](/images/pasted-337.png)

ä¸Šå›¾æ˜¾ç¤ºäº†ä¸€ä¸ªä¾‹å­ï¼Œæ–‡ç« ç”±ä¸¤ä¸ªæ®µè½ç»„æˆï¼Œæ¯ä¸ªæ®µè½æœ‰3ä¸ªå¥å­ã€‚å®çº¿è¡¨ç¤ºå†…éƒ¨è¿æ¥ï¼Œæ˜¯åŒå‘çš„ï¼Œè™šçº¿è¡¨ç¤ºå¤–éƒ¨è¿æ¥ï¼Œæ˜¯æœ‰æ–¹å‘çš„ã€‚Vä»£è¡¨èŠ‚ç‚¹ï¼Œè¡¨ç¤ºå¥å­ã€‚Eè¡¨ç¤ºè¾¹ï¼Œç”¨æ¥è¡¡é‡å¥å­çš„ç›¸ä¼¼åº¦ã€‚



éœ€è¦æ€è€ƒçš„é—®é¢˜ ğŸ¤” â“âŒ›ï¸

1ã€æœ¬æ–‡åŸºäºè¿™æ ·çš„å‡è®¾ï¼šä¸å…¶ä»–å¥å­ç›¸ä¼¼æ€§æ›´å¤§çš„å¥å­ï¼Œæ›´åŠ é‡è¦ï¼Œå°±è¢«é€‰å–ä¸ºæ‘˜è¦äº†ã€‚ä½†æ˜¯è¿™ä¸ªæ€æƒ³æœ¬èº«æ˜¯æ­£ç¡®çš„å—ï¼Ÿ

2ã€sectionçš„è¡¨ç¤ºï¼Œä½¿ç”¨æ‰€æœ‰å¥å­è¡¨ç¤ºå‘é‡çš„å¹³å‡ï¼Œæ˜¯å¦åˆç†ï¼Ÿ



# æ•°æ®é¢„å¤„ç†

è®ºæ–‡ä½¿ç”¨äº†ä¸¤ç§æ•°æ®é›†ï¼Œåˆ†åˆ«æ˜¯ï¼šarXiv (æ²¡ä¸‹è½½) / PubMed (ä¸‹è½½ä½¿ç”¨äº†ï¼Œtrain.txt 4.4Gï¼Œval.txt 251MB)ã€‚ä½†æ˜¯æˆ‘éœ€è¦è¿ç§»åˆ°æˆ‘è‡ªå·±çš„æ•°æ®é›†ä¸Šæ¥ä½¿ç”¨ã€‚



ç»™å‡ºçš„Pubmedçš„æ•°æ®ç»„ç»‡å½¢å¼å¦‚ä¸‹ï¼š

```python
{ 
  'article_id': str,
  'abstract_text': List[str],
  'article_text': List[str],
  'section_names': List[str],
  'sections': List[List[str]]
}
```

æ›´å…·ä½“åœ°è¯´ï¼Œæ˜¯å¦‚ä¸‹æ ¼å¼

```python
{

"article_id": "XXX" ,
"article_text": ["ç¬¬ä¸€å¥è¯", "ç¬¬äºŒå¥è¯", "......"] ,
"abstract_text": ["\<S>bchdbch.\</S> ", "\<S>vbvbsvnjksf.\</S>"] ,  ã€è¿™ä¸ªåé¢è¢«å½“æˆäººå·¥æ ‡è®°çš„æ‘˜è¦çš„ç»“æœå•¦ã€‘
"labels" : null
"section_names", ["1. Introduction", "2. Methods", "3. Results", "4. Discussion"]
"sections": [ ["chjdsna", "cdsnkcd", "cjsfgeuybc" ], [XXX] , [XXX], [XXX] ]
}
```

è‡ªå·±çš„æ•°æ®é›†æ˜¯çº¯æ–‡æœ¬çš„å½¢å¼ï¼Œéœ€è¦æŠŠå®ƒå¤„ç†æˆä¸Šæ–‡çš„æ ¼å¼ã€‚



1ã€ç¼–å†™äº†article2txt()å‡½æ•°ï¼Œç”¨äºå°†æ–‡æœ¬æŒ‰å¥å­åˆ’åˆ†ï¼Œæ¯ä¸ªå¥å­ä¸€è¡Œï¼Œä¿å­˜ä¸ºtxtæ ¼å¼ã€‚

```python
å†…å®¹ï¼š
	è¯»å–æ–‡æœ¬
	åˆ¤æ–­æ˜¯å¦è‹±æ–‡ï¼Œè‹±æ–‡æ‰å†™å…¥
  ä½¿ç”¨('. ') åˆ’åˆ†ä¸ºä¸€å¥ä¸€å¥ã€‚
	å†™å…¥txt
```

ä¸€ä¸ªbugï¼Œå¦‚ä¸‹æ ¼å¼çš„å†…å®¹è¢«åˆ†å¼€äº†ï¼Œéœ€è¦åç»­æ‰‹å·¥è°ƒæ•´

```python
Update (Oct. 30, 2019): Talos is disclosing two additional
```



2ã€æ‰‹å·¥æ ‡è®°æ®µè½

æ¯ä¸ªsectionçš„å¼€å¤´æ˜¯æ®µè½æ ‡é¢˜ï¼›

æ‰‹å·¥è°ƒæ•´æ®µè½ï¼Œæ¯ä¸ªsectionä¹‹é—´ä»¥3ä¸ªæ¢è¡Œç¬¦åˆ†å‰²ã€‚

![upload successful](/images/pasted-338.png)

ä¿å­˜åœ¨å½“å‰çš„txtæ–‡æ¡£ä¸­



3ã€txtè½¬ä¸ºjsonæ–‡ä»¶

å®šä¹‰äº†txt2jsonå‡½æ•°ï¼Œç”¨äºå°†ä¸€å¥ä¸€å¥çš„txtè½¬ä¸ºæ¨¡å‹å¯ä»¥ç›´æ¥è¯»å–çš„jsonæ–‡ä»¶ã€‚

éšåå®šä¹‰äº†exclude_nå‡½æ•°ï¼Œç”¨äºå°†äº§ç”Ÿçš„jsonæ–‡ä»¶çš„æ¢è¡Œç¬¦åˆ æ‰ã€‚



4ã€å°†å¤šä¸ªtxtæ–‡æ¡£åˆå¹¶ä¸ºä¸€ä¸ª

éœ€è¦ä¿è¯æ¯ä¸ªtxtåé¢éƒ½æœ‰ä¸€ä¸ªæ¢è¡Œç¬¦ï½

```python
cat *.txt > test.txt
```



# æ–‡æœ¬è¯»å–

å¤„ç†è¯»å–æ–‡æ¡£å†…å®¹çš„å‡½æ•°ä½äº`dataset_iterators/pubmed.py`

1ã€åœ¨pubmed.pyä¸­ï¼Œä½¿ç”¨dataclassæ¥è£…é¥°PubmedDocï¼Œä½œç”¨æ˜¯æ— éœ€å®šä¹‰\__init__å‡½æ•°ï¼Œå®šä¹‰å‚æ•°å±æ€§æ—¶æ›´åŠ æ–¹ä¾¿ä¸”æ˜“è¯»ã€‚å…·ä½“å¯å‚è€ƒ[è¿™ç¯‡](https://zhuanlan.zhihu.com/p/59657729)ã€‚



2ã€è¯»å–æ–‡æœ¬å†…å®¹

åœ¨PubmedDatasetçš„\__init__å‡½æ•°ä¸­ï¼Œè¯»å–txtæ–‡æ¡£çš„å†…å®¹ï¼Œtxtçš„æ¯ä¸€è¡ŒåŒ…å«äº†ä¸€ç¯‡æ–‡ç« ã€‚å°†æ¯ä¸€è¡Œéƒ½è§£ææˆ PubmedDoc çš„æ ¼å¼ã€‚

å¦‚æœæœ‰é•¿åº¦é™åˆ¶ï¼Œä½¿ç”¨_filter_doc_lenå‡½æ•°è¿›è¡Œè¿‡æ»¤ã€‚ä¼šç›´æ¥æŠŠé•¿åº¦ä¸ç¬¦åˆçš„æ–‡æœ¬ç»™è¿‡æ»¤æ‰ï¼Œä¹Ÿå°±æ˜¯ä¸ä¼šå¯¹ä»–ä»¬è¿›è¡Œæ‘˜è¦ï¼Œè€Œä¸æ˜¯è¿›è¡Œpaddingæ“ä½œã€‚



3ã€å°è£…æ•°æ®

ç”ŸæˆDocumentæ ¼å¼çš„æ•°æ®ã€‚åœ¨punned.pyä¸­æœ‰`from hipo_rank import Document, Section`ï¼Œåœ¨hipo_rankæ–‡ä»¶å¤¹ä¸‹çš„ \__init__.py ä¸­æ‰¾åˆ°äº†å¦‚ä¸‹å®šä¹‰ã€‚

```python
@dataclass
class Section:
    # dataclass wrapper for section in a document and associated text (split into sentences)
    id: str # section name
    sentences: List[str]
    meta: Optional[Dict] = None

@dataclass
class Document:
    # dataclass wrapper for documents yielded by a dataset iterator
    sections: List[Section]
    reference: List[str]
    meta: Optional[Dict] = None
```



exp3_run.pyä¸­è°ƒç”¨çš„æ ¼å¼ä¸º

```python
DataSet = dataset(**dataset_args)

In [4]: dataset_id
Out[4]: 'pubmed_test'

In [5]: dataset
Out[5]: hipo_rank.dataset_iterators.pubmed.PubmedDataset

In [6]: dataset_args
Out[6]: {'file_path': 'data/pubmed-release/test4.txt'}
```



# è¯åµŒå…¥

æœ‰å››ç§æ¨¡å‹å¯é€‰ï¼Œåˆ†åˆ«æ˜¯`rand` ã€`bert`ã€`w2v`å’Œ`sent_transformers`ã€‚

ä»¥[sentence-transformers](https://github.com/UKPLab/sentence-transformers) ä¸ºä¾‹æ¥è¯´æ˜è°ƒç”¨è¿‡ç¨‹ï¼Œ(å› ä¸ºå…¶ä»–çš„å‡ ä¸ªæ¨¡å‹éƒ½æœ‰ç‚¹å¤ªå¤§äº†)

å®‰è£…ï¼špip install -U sentence-transformers

ä½¿ç”¨ç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
'''
æœ€å¼€å§‹åœ¨æœ¬æœºåŠ è½½modelå¤±è´¥ï¼Œåœ¨æœåŠ¡å™¨è¿è¡ŒæˆåŠŸï¼Œå¯èƒ½æ˜¯ç¯å¢ƒé—®é¢˜å¯¼è‡´çš„ã€‚
'''
sentences = ['This framework generates embeddings for each input sentence',
    'Sentences are passed as a list of string.', 
    'The quick brown fox jumps over the lazy dog.']
sentence_embeddings = model.encode(sentences)
```



1ã€å¯¼å…¥æ•°æ®ç±»å‹

```python
from hipo_rank import Document, Embeddings, SectionEmbedding, SentenceEmbeddings
```

å…¶å®šä¹‰å¦‚ä¸‹

```python
@dataclass
class Document:
    # dataclass wrapper for documents yielded by a dataset iterator
    sections: List[Section]
    reference: List[str]
    meta: Optional[Dict] = None

@dataclass
class SentenceEmbeddings:
    # dataclass wrapper for section in a document and associated sentence embeddings
    id: str # section name
    embeddings: ndarray # first dim = number of sentences
    meta: Optional[Dict] = None
      
@dataclass
class SectionEmbedding:
    # dataclass wrapper for section in a document and associated embedding
    id: str # section name
    embedding: ndarray
    meta: Optional[Dict] = None
      
@dataclass
class Embeddings:
    # dataclass wrapper for section in a document and associated sentence embeddings
    sentence: List[SentenceEmbeddings]
    section: List[SectionEmbedding]
    meta: Optional[Dict] = None
```



2ã€embeddingè®¡ç®—

sentenceæ˜¯è¾ƒä¸ºå¸¸è§„çš„è®¡ç®—æ–¹å¼

```python
np.stack(self.model.encode(sentences, show_progress_bar=False))
```

sectionçš„è®¡ç®—å¦‚ä¸‹ï¼š

```python
section_embeddings = [SectionEmbedding(id=se.id, embedding=np.mean(se.embeddings, axis=0))
                              for se in sentence_embeddings]
```

å°†ä¸€ä¸ªsectionçš„æ‰€æœ‰sentenceå–å¹³å‡å€¼ï¼Œå°±æ˜¯ä¸€ä¸ªsectionçš„embeddingçš„å€¼ã€‚



# è¾¹çš„æƒé‡åŠæ–¹å‘è®¡ç®—

å›¾çš„è¿æ¥å½¢å¼åˆ†ä¸ºä¸¤ç±»ï¼šå†…éƒ¨è¿æ¥å’Œå¤–éƒ¨è¿æ¥

å†…éƒ¨è¿æ¥ï¼šä¸€ä¸ªsectioné‡Œé¢çš„æ‰€æœ‰sentenceéƒ½ä¸¤ä¸¤ç›¸è¿ã€‚

å¤–éƒ¨è¿æ¥ï¼Œåªå…è®¸è®©æŸä¸€ä¸ªsectioné‡Œçš„ä¸€ä¸ªtopèŠ‚ç‚¹ æŒ‡å‘å¦å¤–çš„sectionçš„èŠ‚ç‚¹ã€‚ç›®çš„æ˜¯å‡å°‘è¾¹çš„æ•°é‡ã€‚



1ã€ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—

hipo_rankä¸‹çš„similaritiesç”¨æ¥è®¡ç®—å¥å­é—´çš„ç›¸ä¼¼åº¦ï¼Œæœ¬æ–‡ä½¿ç”¨çš„æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œ`CosSimilarity`ç±»ä¼ å…¥çš„å‚æ•°æ˜¯é˜ˆå€¼ã€‚

è¯¥ç±»ä¸‹çš„`get_similarities`è®¡ç®—äº†sentenceå’Œsentenceã€sentenceå’Œsectionã€sectionå’Œsectionä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚

åœ¨cos.pyæ–‡ä»¶ä¸­è®¾ç½®çš„è¾¹éƒ½æ˜¯undirectedçš„ã€‚

```python
directions = ["undirected" for _ in pair_indices]
```



2ã€æ–¹å‘è®¡ç®—

åŸºæœ¬æ€æƒ³ï¼šæœ€é‡è¦çš„å¥å­éƒ½åœ¨å¼€å¤´å’Œç»“å°¾å‡ºç°ã€‚å®šä¹‰äº†æŒ‡æ ‡dbæ¥è¡¡é‡è¿™ç§é‡è¦æ€§ã€‚

![upload successful](/images/pasted-339.png)

directionsä¸‹çš„edge.pyå®šä¹‰äº†EdgeBasedç±»ã€‚

pythonä»£ç å®ç°å¦‚ä¸‹

```python
    def _get_direction(self,  node1_index: int, node2_index: int, num_nodes: int) -> str:
        dist_i = min(node1_index, self.u * (num_nodes - node1_index - 1))
        dist_j = min(node2_index, self.u * (num_nodes - node2_index - 1))
        if dist_i > dist_j:
            return "forward"
        elif dist_i < dist_j:
            return "backward"
        else:
            return "undirected"
```

ç®­å¤´çš„æ–¹å‘æ˜¯ä»æƒé‡å¤§çš„æŒ‡å‘æƒé‡è¾ƒå°çš„ã€‚forwardä»£è¡¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„ä½ç½®æƒé‡æ¯”ç¬¬äºŒä¸ªèŠ‚ç‚¹å¤§ã€‚



å¼•å…¥äº†

```python
@dataclass
class Similarities:
    # dataclass wrapper for similarities in a document
    sent_to_sent: List[SentenceSimilarities]
    sect_to_sect: SectionSimilarities
    sent_to_sect: List[SentenceSimilarities]
    meta: Optional[Dict] = None
```



éšååœ¨edgeä¸­è¿˜æœ‰å®šä¹‰çš„update_directionså‡½æ•°ï¼Œç”¨æ¥æ›´æ–°è¾¹çš„æƒé‡ä¿¡æ¯ã€‚

order.pyçš„æ„é€ å’Œedge.pyå·®ä¸å¤šï½ä½†æ˜¯å…¶ä½ç½®ä¿¡æ¯åªä½¿ç”¨äº†ç»å¯¹è·ç¦»ã€‚

undirected.pyï¼Œå‡½æ•°å†…éƒ¨è¿›è¡Œå¾ªç¯ï¼Œæ ‡æ³¨æ˜¯undirectedçš„ï½



3ã€scoreè®¡ç®—

éšååœ¨`hipo_rank/scorers`ç›®å½•ä¸‹è¿›è¡Œæƒé‡çš„æœ€ç»ˆè®¡ç®—

ä»\__init__.pyå¼•å…¥å¦‚ä¸‹å†…å®¹

```python
score = float
section_idx = int
local_idx = int
global_idx = int

Scores = List[Tuple[score,section_idx,local_idx,global_idx]]
```



å®šä¹‰äº†MultiplyScorerç±»

```python
class MultiplyScorer:
  def __init__:
    '''
    æ‰€éœ€å‚æ•°ï¼š
    forward_weight: float = 0.,
    backward_weight: float = 1.,
    section_weight: float = 1.,
    '''
    self.forward_sent_to_sent_weight = forward_weight #å‰å‘çš„ ä¸€ä¸ªsectionä¹‹é—´ï¼Œå¥å­åˆ°å¥å­çš„æƒé‡ã€‚è¿™é‡Œå…¶å®å°±æ˜¯å‚æ•°Î»1
    self.backward_sent_to_sent_weight = backward_weight
    
    
  	self.forward_sect_to_sect_weight = forward_weight * section_weight #sectionçš„æƒé‡å€¼æ˜¯ä¸¤è€…ç›¸ä¹˜
  	self.backward_sect_to_sect_weight = backward_weight * section_weight
  
  
  def get_scores():
    '''
    ç”¨æ¥è®¡ç®—scoreçš„å¾—åˆ†
    '''
    wI=Î»1 âˆ—sim(vI,vI), ifdb(vI)â‰¥db(vI),
    æˆ– wI=Î»2âˆ—sim(vjI,viI), ifdb(viI)<db(vjI)
    
    scores[sect_index][i] += self.forward_sent_to_sent_weight * sim / norm_factor
    scores[sect_index][j] += self.backward_sent_to_sent_weight * sim / norm_factor
    
```



éšåè®¡ç®—æ¯ä¸ªå¥å­æ€»å…±çš„æƒé‡ï¼Œæ˜¯å…¶å†…éƒ¨è¿æ¥å’Œå¤–éƒ¨è¿æ¥çš„å’Œã€‚

åœ¨add.pyä¸­ï¼Œå…ˆè®¡ç®—sent_to_sentçš„scoreå€¼ï¼Œå†è®¡ç®—sent_to_sectçš„å€¼ï¼Œä¸¤è€…ç›¸åŠ ï¼Œæ±‚çš„å’Œçš„å€¼å­˜å‚¨åœ¨`scores[sect_index][i]` ä¸­ã€‚

```python

        # add sent_to_sent scores
        for sect_index, sent_to_sent in enumerate(similarities.sent_to_sent):
            pids = sent_to_sent.pair_indices
            dirs = sent_to_sent.directions
            sims = sent_to_sent.similarities
            norm_factor = len(scores[sect_index]) - 1
            for ((i,j), dir, sim) in zip(pids, dirs, sims):
                if dir == "forward":
                    scores[sect_index][i] += self.forward_sent_to_sent_weight * sim / norm_factor * self.sent_to_sent_weight
                    scores[sect_index][j] += self.backward_sent_to_sent_weight * sim / norm_factor * self.sent_to_sent_weight
                elif dir == "backward":
                    scores[sect_index][j] += self.forward_sent_to_sent_weight * sim / norm_factor * self.sent_to_sent_weight
                    scores[sect_index][i] += self.backward_sent_to_sent_weight * sim / norm_factor * self.sent_to_sent_weight
                else:
                    scores[sect_index][j] += self.backward_sent_to_sent_weight * sim / norm_factor * self.sent_to_sent_weight
                    scores[sect_index][i] += self.backward_sent_to_sent_weight * sim / norm_factor * self.sent_to_sent_weight

        # add sent_to_sect scores
        for sect_index, sent_to_sect in enumerate(similarities.sent_to_sect):
            pids = sent_to_sect.pair_indices
            dirs = sent_to_sect.directions
            sims = sent_to_sect.similarities
            norm_factor = len(scores)
            for ((i,j), dir, sim) in zip(pids, dirs, sims):
                if dir == "forward":
                    scores[sect_index][i] += self.forward_sent_to_sect_weight * sim / norm_factor * self.sent_to_sect_weight
                elif dir == "backward" or dir == "undirected":
                    scores[sect_index][i] += self.backward_sent_to_sect_weight * sim / norm_factor * self.sent_to_sect_weight

```



éšåå°†ç»“æœå­˜å‚¨åœ¨ranked_scoresä¸­ï¼Œå¹¶æŒ‰ç…§ç›¸ä¼¼åº¦çš„å€¼è¿›è¡Œä»é«˜åˆ°ä½æ’åºï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
        ranked_scores = []
        sect_global_idx = 0
        for sect_idx, sect_scores in enumerate(scores):
            for sent_idx, sent_score in enumerate(sect_scores):
                ranked_scores.append(
                    (sent_score,
                     sect_idx,
                     sent_idx,
                     sect_global_idx + sent_idx
                     )
                )
            sect_global_idx += len(sect_scores)
        ranked_scores.sort(key=lambda x: x[0], reverse=True)
```



æ–‡ä¸­ç»™å‡ºçš„è®¡ç®—å…¬å¼å¦‚ä¸‹å›¾ï¼Œè¿™é‡Œåªç»™äº†addçš„è®¡ç®—æ–¹å¼ï¼Œæ²¡æœ‰ç»™å‡ºç›¸ä¹˜çš„è¦å¦‚ä½•è®¡ç®—ã€‚

![upload successful](/images/pasted-340.png)

multiplyçš„æ–¹æ³•ä¸å…¶å¤§è‡´ç›¸åŒï¼Œä½†æ˜¯è®¡ç®—äº†ä¹˜ç§¯ï¼Œè€Œä¸æ˜¯æ±‚å’Œã€‚sent_to_sentçš„å€¼å­˜å‚¨åœ¨`scores[sect_index][i]`ä¸­ï¼Œsect_to_sectçš„å€¼å€¼å­˜å‚¨åœ¨`sect_scores[i]`ä¸­ã€‚éšåå°†

```python
        for i, score in enumerate(sect_scores):
            scores[i] = [score*s for s in scores[i]]
```

å°†åŸæœ¬çš„`scores[sect_index][i]`ä¸­çš„æ¯ä¸ªsentenceä¹˜ä¸Š å…¶æ‰€åœ¨çš„sectionå¯¹åº”çš„sect_scoresçš„å€¼ã€‚



# ç”Ÿæˆæ‘˜è¦

åœ¨ `hipo_rank/summarizers` æ–‡ä»¶ä¸‹ã€‚

default.py ä¸­çš„DefaultSummarizer æŒ‰ç…§rankåçš„é¡ºåºï¼Œæ¥ç»„ç»‡æŒ‘é€‰å‡ºçš„å¥å­ã€‚





# å®éªŒç»“æœ



1ã€ROUGE å®‰è£…ä½¿ç”¨

å°†ROUGE-1.5.5æ–‡ä»¶å¤¹ä¸Šä¼ 



ç”±äºä¹‹å‰åœ¨å¦ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹å®‰è£…è¿‡ï¼Œç›´æ¥ä½¿ç”¨ä¼šæŠ¥é”™

`PermissionError: [Errno 13] Permission denied: 'xxx/acl18/ROUGE-1.5.5/ROUGE-1.5.5.pl'`



å‚è€ƒ https://blog.csdn.net/qingjuanzhao/article/details/106076323ï¼Œ è¿›è¡Œå¦‚ä¸‹è®¾ç½®

`pyrouge_set_rouge_path ~/mayue/hipo/ROUGE-1.5.5`

ä½†æ˜¯ä½¿ç”¨æ—¶è¿˜æ˜¯æœ‰æŠ¥é”™ï¼Œ`PermissionError: [Errno 13] Permission denied: '/home/g3/mayue/hipo/ROUGE-1.5.5/ROUGE-1.5.5.pl'`

å†å‚è€ƒ https://github.com/lancopku/Global-Encoding/issues/13ï¼Œè¿è¡Œ`chmod 777 ROUGE-1.5.5.pl`ï¼Œå³å¯è§£å†³è¯¥é—®é¢˜ã€‚



2ã€exp1.py

ä½¿ç”¨PubMedçš„éªŒè¯é›† æ¥å¾®è°ƒè¶…å‚æ•°ï¼Œå¹¶ä¸”è¿›è¡Œæ¶ˆèå®éªŒã€‚

![upload successful](/images/pasted-341.png)


ä½œè€…å¾—å‡ºè¶…å‚æ•°ç»“æœï¼š

Î»1 = 0.0, Î»2 = 1.0, Î± = 1.0, with Î¼1 = 0.5

Î± æ§åˆ¶å¼€å§‹ä½ç½®å’Œç»“å°¾çš„ä½ç½® å“ªä¸ªæ›´é‡è¦ã€‚



3ã€è¿è¡Œexp3ï¼Œç”Ÿæˆæ‘˜è¦

è¿›è¡Œæµ‹è¯•ï¼Œç”Ÿæˆæ‘˜è¦ï¼Œå¹¶ä¸”è®¡ç®—ROUGEå€¼ã€‚

è¯åµŒå…¥éƒ¨åˆ†é€‰å–äº†5ä¸ªæ¨¡å‹æ¥å®éªŒï¼Œåˆ†åˆ«å¦‚ä¸‹

```python
rand_200
biomed_w2v
pacsum_bert
st_bert_base
st_roberta_large
```



ä½¿ç”¨Embedder.get_embeddings(doc)ï¼Œä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆä¸€ä¸ªembed

![upload successful](/images/pasted-342.png)

æ ¹æ®å®šä¹‰çš„è®¡ç®—ç›¸ä¼¼æ€§çš„æ–¹å¼ï¼Œå¾—åˆ°sims

sentence-sentence

section-sentence

sect_to_sect

ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¹¶ä¸”æ˜¯ä¸å¯¹ç§°çš„ï¼Œå¥å­é—´çš„ä¸å¯¹ç§°ã€‚

![upload successful](/images/pasted-343.png)

å†æ ¹æ®DIRECTIONSï¼Œå¾—åˆ°æ˜¯edgeè¿˜æ˜¯orderè¿˜æ˜¯undirected

æ›´æ–°simsçš„æ–¹å‘

è®¡ç®—å¾—åˆ†ï¼Œå¾—åˆ°æ‘˜è¦ï¼Œä¿å­˜ã€‚ä¿å­˜çš„ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤º

![upload successful](/images/pasted-344.png)

æ¯ä¸ªå¥å­åé¢è·Ÿç€çš„å››ä¸ªæ•°å­—åˆ†åˆ«å¦‚ä¸‹ï¼š

```python
                ranked_scores.append(
                    (sent_score, #å¥å­çš„å¾—åˆ†
                     sect_idx, #æ‰€åœ¨sectionçš„ç´¢å¼•
                     sent_idx, #æ‰€åœ¨å¥å­çš„ç´¢å¼•
                     sect_global_idx + sent_idx #å…¨å±€ç´¢å¼•
                     )
```



4ã€baselineæ¯”è¾ƒ

è¯„ä¼°æŒ‡æ ‡ï¼šROUGE F1 / ROUGE- 1 / ROUGE-2 / ROUGE-L



æ— ç›‘ç£æŠ½å–å¼ï¼šSumBasic(2007)ã€LSA(2004)ã€LexRank(2004)ã€PACSUM(2019) (PACSUMï¼ŒåŸºäºå›¾çš„æ— ç›‘ç£æ¨¡å‹ï¼Œåˆ©ç”¨äº†åŸºäºBERTçš„å¥å­è¡¨ç¤ºã€‚)

æœ‰ç›‘ç£ç¥ç»ç½‘ç»œæŠ½å–å¼ï¼švanilla encoder-decoderæ¨¡å‹(2016)ã€SummaRuNNer(2017)ã€GlobalLocalCont(2019)ã€Sent-CLF(2019)å’ŒSent-PTR(2019)

ç¥ç»ç½‘ç»œç”Ÿæˆå¼ï¼šAttn-Seq2Seq(2016)ã€Pntr- Gen-Seq2Seq(2017)ã€Discourse- aware(2018)

å…¶ä»–ï¼šfirst k tokens(no paper)ã€Oracle(2017)



5ã€äººå·¥è¯„ä¼°

ä¹‹å‰æ²¡æœ‰å·¥ä½œåšä¸ªè¿™ä¸ªæ–¹é¢ã€‚



# ä½¿ç”¨æ‰‹å†Œ



## æ•°æ®çˆ¬è™«

å¾…åš

æ—¢ç„¶æœ‰é‚£äº›æ•°æ®ï¼Œè¯´æ˜ä¹‹å‰è‚¯å®šæœ‰äººçˆ¬å–è¿‡äº†ï¼Œæˆ‘æ²¡å¿…è¦å†é‡å¤é€ è½®å­è¾½~



## æ–‡æœ¬å†…å®¹è§£æ

è¿è¡Œparsefile_1.pyï¼Œå°†ç›®å½•ä¸‹çš„æ–‡æœ¬è¿›è¡Œè§£æã€‚

è¿è¡Œparsefile_2.pyï¼Œå°†æ–‡æœ¬æ”¹ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ¨¡å¼ã€‚



`cat *.txt >text4.txt`



## æ¨¡å‹è¿è¡Œ



å¯¹äºå®‰å…¨æ–‡ç« æ¥è¯´ï¼Œä¸€èˆ¬æœ€å¼€å§‹çš„ä½ç½®æ›´åŠ é‡è¦ï¼Œæ‰€ä»¥aè®¾ç½®ä¸º1.2ï¼Œå°±ä½¿å¾—æœ«å°¾ä½ç½®çš„æƒé‡æ›´å¤§äº†ï¼Œæœ«å°¾ä½ç½®å°±ä¼šå˜å¾—æ›´åŠ ä¸é‡è¦ã€‚ä¿®æ”¹`directions/edge.py`



å°†ä¸Šæ–‡å¾—åˆ°çš„text4.txt æ”¾å…¥æœåŠ¡å™¨ä¸­ï¼Œè¿è¡Œexp3_run.pyï¼Œå¾—åˆ°çš„ç»“æœå­˜å‚¨åœ¨ `results/exp3`ä¸­ã€‚



## è§£æsummary

è™½ç„¶ä¸Šæ–‡ç”Ÿæˆäº†æ‘˜è¦æ–‡æ¡£ï¼Œä½†æ˜¯å¥å­æ˜¯ä»¥ scoreçš„å¾—åˆ†é¡ºåºæ¥ç»„ç»‡çš„ï¼Œå¹¶ä¸”ä¹Ÿä¸æ˜¯äººç›´æ¥å¯è¯»çš„å½¢å¼ã€‚ä»¥å¦‚ä¸‹æ–¹å¼è¿›è¡Œæ’åº

```python
pubmed_test_hiporank_path = '/Users/my/Documents/blog/source/_postsresults/exp3/pubmed_test-st_bert_base-cos-edge-add_f=0.0_b=1.0_s=0.5/summaries.json'
pubmed_test_hiporank_ori = json.loads(Path(pubmed_test_hiporank_path).read_text())

pubmed_test_hiporank_txt = ["\n".join([s[0].replace("\n", "") for s in sorted(x['summary'], key=lambda x:x[4])]) for x in pubmed_test_hiporank_ori]
```



æ­¤å¤–ï¼Œä¸ºäº†å¢åŠ å¯è¯»æ€§ï¼Œéœ€è¦å°†æ–‡ç« çš„æ ‡é¢˜åŠ ä¸Šã€‚

åœ¨init.pyçš„`Document`ä¸­åŠ å…¥titleï¼ŒåŠ å…¥åå…¶æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š

```python
@dataclass
class Document:
    # dataclass wrapper for documents yielded by a dataset iterator
    sections: List[Section]
    reference: List[str]
    title: str
    meta: Optional[Dict] = None
```

åœ¨pubmed.pyä¸­åŠ å…¥title

```python
	title = [doc.article_id for doc in docs]
	return [Document(sections=s, reference=r, title=t) for s,r,t in zip(sections, references, title)]
```

æœ€ååœ¨è¿è¡Œexp3_run.pyçš„éƒ¨åˆ†ï¼Œå°†titleå†™å…¥

```python
results.append({
	"num_sects": len(doc.sections),
  "num_sents": sum([len(s.sentences) for s in doc.sections]),
	"summary": summary,
	"title": doc.title
	})
```



æœ€ç»ˆæ¯ç¯‡æ–‡ç« å¾—åˆ°çš„ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤º

![upload successful](/images/pasted-345.png)

å¯ä»¥çœ‹å‡ºä»ç„¶æœ‰è¾ƒå¤šçš„IOCæŒ‡æ ‡æˆ–URLå‚è€ƒé“¾æ¥ï¼Œåç»­éœ€è¦å°†å…¶åˆ é™¤ã€‚





# TODO âŒ›ï¸



1ã€æµ‹è¯•æ—¶ï¼Œåˆ†åˆ«ä½¿ç”¨addå’Œmultiplyçš„æ–¹æ³•æ¥è¿›è¡Œå°è¯•ï¼ŒæŸ¥çœ‹ç»“æœ



2ã€ä¸ä»¥ä¸€ä¸ªå®Œæ•´çš„å¥å­æ¥åˆ¤æ–­ç›¸ä¼¼åº¦ï¼Œè€Œæ˜¯ç”¨å«æœ‰æ ‡ç‚¹ç¬¦å·çš„ç‰‡æ®µã€‚æ¯”å¦‚é‡åˆ°äº†ä¸€ä¸ªé€—å·ï¼Œå°±å°†å…¶è¿›è¡Œåˆ‡å‰²ã€‚



3ã€ä»ç„¶æ‰‹å·¥æ ‡è®°sectionï¼ï¼ï¼



4ã€å…ˆæå–IOCï¼Œå°†æå–å‡ºçš„IOCä»æ–‡æœ¬ä¸­åˆ é™¤æ‰ï¼Œå†è¿›è¡Œæ‘˜è¦



5ã€å°†è‹±æ–‡è½¬ä¸ºä¸­æ–‡