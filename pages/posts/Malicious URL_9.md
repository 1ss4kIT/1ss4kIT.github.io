---
title: æ¶æ„URLæ£€æµ‹(ä¹)ä¸€äº›æ¯”è¾ƒæ‚çš„æ–‡ç« 
date: 2021-04-27 08:02:42
tags:
categories:
    æ¶æ„URLæ£€æµ‹
---

## äºŒåå››ã€è®­ç»ƒTransformers

1ã€å‘è¡¨äºç«çœ¼ä¸Šçš„ä¸€ç¯‡æ–‡ç« ï¼Œåœ°å€https://www.fireeye.com/blog/threat-research/2021/01/training-transformers-for-cyber-security-tasks-malicious-url-prediction.htmlï¼Œ
æ—¶é—´2021å¹´1æœˆã€‚


2ã€å‰äººå·¥ä½œ

GPT-3æ¨¡å‹å¯ä»¥äº§ç”Ÿè¯­æ³•æ­£ç¡®çš„é•¿segments

...ç­‰ç­‰ç­‰

ä½†æ˜¯å±å®³æ˜¯ï¼šå¯èƒ½äº§ç”Ÿå¹¶ä¼ æ’­å¾ˆå¤šæ— æ„ä¹‰çš„ä¿¡æ¯



3ã€æ“ä½œæ˜¯åœ¨å­—ç¬¦levelï¼Œæ¯ä¸€ä¸ªå­—ç¬¦ä¸ä¸€ä¸ªinput tokenç›¸å…³ã€‚

æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š

![image-20210422085212777](/Malicious_URL_9/image-20210422085212777.png)



4ã€æŸå¤±å‡½æ•° & è®­ç»ƒRegimes

Transformerså…è®¸è‡ªç›‘ç£è®­ç»ƒ(å¯ä»¥çœ‹æˆæ˜¯æ— ç›‘ç£çš„å­¦ä¹ )ï¼Œå¯ä»¥å°†transformerçœ‹ä½œæ˜¯ç‰¹å¾æå–å™¨ã€‚

regimes:

(1) Direct Label Prediction (Decode-To-Label):   transformwesæå–ç‰¹å¾ï¼Œç„¶åè¾“å…¥åˆ°åˆ†ç±»å™¨è®¡ç®—ã€‚ä½¿ç”¨äºŒå€¼äº¤å‰ç†µæŸå¤±ã€‚

(2) Next-Character Prediction Pre-Training and Fine-Tuningï¼šç±»ä¼¼é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œå¯ä»¥åˆ©ç”¨æ— æ ‡ç­¾çš„æ•°æ®ã€‚ç”±äºæ˜¯å¤šåˆ†ç±»ï¼Œå¿…é¡»ç”¨softmaxå‡½æ•°ã€‚

(3) Balanced Mixed-Objective Trainingï¼šä½¿ç”¨åˆ°æ›´å¤šçš„çŸ¥è¯†ã€‚æœ¬æ–‡åŒæ—¶è®­ç»ƒäºŒå€¼åˆ†ç±»å’Œä¸‹ä¸ªç‰¹å¾åˆ†ç±»ã€‚æŸå¤±å‡½æ•°ä¹Ÿæ˜¯äºŒè€…çš„é›†åˆã€‚



5ã€å®éªŒç»“æœ

å®éªŒå‘ç°ï¼ŒBalanced Mixed-Objective Trainingæ•ˆæœæœ€å¥½ï¼ŒDirect Label Predictionæ¬¡ä¹‹ã€‚

ç”±ä¸å…¶ä»–æ¨¡å‹å¦‚RFã€CNNã€LSTMç­‰ã€‚

ç»“æœæ˜¯CNNæ•ˆæœæœ€å¥½ï¼Œå¾—å‡ºç»“è®ºTransformerå¯èƒ½ä¸æ˜¯é‚£ä¹ˆé€‚ç”¨äºæ¶æ„URLæ£€æµ‹ã€‚ã€‚ã€‚å•Šè¿™ =ã€‚=



æ”¶è·ï¼š

1ã€å€Ÿé‰´è¿™ç§ï¼šåŒæ—¶ä½¿ç”¨äºŒå€¼åˆ†ç±»å’Œä¸‹ä¸ªç‰¹å¾åˆ†ç±»çš„æ€æƒ³ã€‚

ä½†æ˜¯æœ¬æ–‡å¾—å‡ºçš„ç»“è®ºæ˜¯ï¼šä¸‹ä¸ªç‰¹å¾åˆ†ç±»å…¶å®å’Œæ¶æ„URLæ£€æµ‹æ²¡æœ‰å¤ªå¤§å…³ç³»ï¼Œæ‰€ä»¥å¯ä»¥è€ƒè™‘ä¸€äº›å…¶ä»–åˆé€‚çš„æ–¹æ³•ã€‚ä½†æ˜¯æŸå¤±å‡½æ•°ä¸­å¼•å…¥ä»–ï¼Œå´æé«˜äº†æ€§èƒ½ã€‚



## äºŒåäº”ã€åŒºå—é“¾ é’“é±¼æ£€æµ‹

åŸæ–‡é¢˜ç›®ï¼šPhishing Scam Detection on Ethereum: Towards Financial Security for Blockchain.

(å…¶å®æœ¬æ–‡ä¸æ˜¯å¤ªNLP)

1ã€æ–‡ç« å‘è¡¨äºIJCAI2020ï¼Œå±äºCCF- Aç±»ä¼šè®®ã€‚ä½œè€…æ¥è‡ªä¸­å±±å¤§å­¦ã€‚

2ã€åœ¨åŒºå—é“¾é¢†åŸŸï¼Œä¸ä»…æœ‰çªƒå–éšç§æ•°æ®ï¼Œè¿˜æœ‰è¯±éª—ç”¨æˆ·å¾€ç‰¹å®šè´¦æˆ·èµšé’±ã€‚éæ³•åŠ å¯†çš„è´§å¸éœ€è¦é€šè¿‡äº¤æ˜“æ‰èƒ½å˜æˆåˆæ³•è´§å¸ã€‚å…¬å…±åŒºå—é“¾çš„äº¤æ˜“è®°å½•æ˜¯å¯ä»¥å…¬å¼€è®¿é—®çš„ï¼Œå¯èƒ½é€ æˆé’“é±¼æ”»å‡»ã€‚

3ã€åŒºå—é“¾ä¸­å®šä½é’“é±¼è´¦æˆ·å­˜åœ¨çš„é—®é¢˜ï¼š

1) åªæœ‰äº¤æ˜“è®°å½•ï¼Œå¯¹è´¦æˆ·çš„ä¿¡æ¯äº†è§£è¾ƒå°‘ã€‚

2) é’“é±¼åœ°å€è¾ƒå°‘ï¼Œè€Œå…¶ä»–åœ°å€éå¸¸å¤šï¼Œå®šä½é’“é±¼åœ°å€å°±æ˜¾å¾—æ¯”è¾ƒå›°éš¾ã€‚

4ã€æœ¬æ–‡æ–¹æ³•ï¼š

![frame](/Malicious_URL_9/frame.jpg)

æœ¬æ–‡æ˜¯åŸºäºå¯¹åŒºå—é“¾çš„å¤„ç†å»æ£€æµ‹é’“é±¼ç”¨æˆ·ã€‚æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„ç€‘å¸ƒç‰¹å¾æå–æ–¹æ³•ï¼Œä»¥åŠä¸€ç§åŸºäºlightGBMçš„åŒé‡‡æ ·åµŒå…¥ç®—æ³•å»æ„å»ºæ¨¡å‹ã€‚



5ã€ç€‘å¸ƒç‰¹å¾æå–æ–¹æ³•

åŸºäºäº¤æ˜“è®°å½•åˆ›å»ºä¸€ä¸ªæœ‰å‘å›¾TGã€‚è¾¹æœ‰ä¸¤ä¸ªå±æ€§ï¼šblocknumberå’Œamoutï¼Œè¡¨ç¤ºè¾¹å‡ºç°çš„æ—¶é—´ å’Œ äº¤æ˜“çš„æ•°é‡ã€‚

![2fig2](/Malicious_URL_9/2fig2.jpg)

æå–Açš„2é˜¶æœ‹å‹çš„ç‰¹å¾ï¼Œæœ‰å¦‚ä¸‹3ä¸ªæ­¥éª¤ï¼š

(1) ä½¿ç”¨äº¤æ˜“è®°å½•ï¼Œè®¡ç®—2é˜¶æœ‹å‹çš„ç»Ÿè®¡ç‰¹å¾ã€‚

(2) ä½¿ç”¨(1)çš„ç»“æœï¼Œè®¡ç®—1é˜¶æœ‹å‹çš„ç»Ÿè®¡ç‰¹å¾ã€‚è€Œä¸æ˜¯ç”¨1é˜¶æœ‹å‹æœ¬èº«çš„æ•°æ®æ¥è®¡ç®—çš„ã€‚

(3) ä½¿ç”¨(2)çš„ç»“æœè®¡ç®—AèŠ‚ç‚¹çš„ç»Ÿè®¡ä¿¡æ¯ã€‚

æ­¤å¤„æ²¡æœ‰è€ƒè™‘è¾¹çš„æ–¹å‘ï½ä½†æ˜¯è¾¹çš„ä¿¡æ¯æ˜¯å¾ˆé‡è¦çš„ï¼Œæ‰€ä»¥åœ¨ä¸¤ä¸ªæ–¹å‘ä¸Šåˆ†åˆ«æå–ç‰¹å¾ã€‚



**ç»“ç‚¹ç‰¹å¾ï¼š**æ˜¯è¿™ä¸ªèŠ‚ç‚¹çš„ç»Ÿè®¡æ•°æ®ã€‚æœ‰ä¸¤ç§ç±»å‹çš„æ•°æ®ï¼šäº¤æ˜“æ•°é‡å’Œäº¤æ˜“æ—¶é—´ã€‚ç‰¹å¾çš„å‘½åæ ¼å¼ï¼šdirection_type_methodã€‚ä¾‹å¦‚ï¼šdirection: in / outï¼Œtype: block / ï¼Œmethodï¼šstd

æœ€åä¸€å…±å¾—åˆ°äº†19ä¸ªç‰¹å¾ï¼š

äº¤æ˜“æ—¶é—´ï¼š

span: ptp

standard deviation: sd

äº¤æ˜“æ•°é‡ï¼š

sum

max

min

mean

standard deviation

äº¤æ˜“ä¸ç›¸å…³ä¿¡æ¯

count(äº¤æ˜“æ•°é‡)

unique(äº¤æ˜“åŒæ–¹)

ä»¥ä¸Šç‰¹å¾éƒ½æ˜¯åœ¨in/outæ–¹å‘ä¸Šåˆ†åˆ«è®¡ç®—ï¼Œå°±æ˜¯2*(2+5+2)=19

æœ€åå†ç®—ä¸€ä¸ªunique_ratio = unique/countã€‚å…±19ä¸ªç‰¹å¾

**æœ‹å‹èŠ‚ç‚¹ç‰¹å¾**

æœ¬æ–‡åªè®¡ç®—ä¸€é˜¶æœ‹å‹èŠ‚ç‚¹ã€‚å‘½åå¦‚ä¸‹:friend_direction_statistic2_statistic1ã€‚
æœ€åå¾—åˆ°2*2 *2 * 5 *5= 200ä¸ªç‰¹å¾ã€‚

6ã€åŒé‡‡æ ·åµŒå…¥ç®—æ³•æ¡†æ¶

(1) åŸºç¡€æ¨¡å‹

gradient boosting decision tree (GBDT)ï¼ŒåŒ…æ‹¬XGBoost å’Œ lightGBMã€‚æœ¬æ–‡å‘ç°åœ¨é’“é±¼æ£€æµ‹æ–¹é¢ï¼ŒlightGBMæ›´åŠ æœ‰æ•ˆï¼Œé€‰æ‹©å®ƒä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚

å…·ä½“æ¨¡å‹çš„ä¸€äº›å®ç°æ­¤å¤„ä¸æ€»ç»“äº†ï¼Œä¹Ÿçœ‹ä¸æ‡‚ TT

ä»è®­ç»ƒé›†ä¸­ç§»é™¤æ‰è¾ƒå°çš„æ¢¯åº¦samplesã€‚å‚è€ƒGOSSã€‚

æ„å»º CART regression treeçš„æ—¶å€™ï¼ŒlightGBMç»‘å®šäº’æ–¥çš„ç‰¹å¾ï¼Œå¯ä»¥å‡å°‘ç‰¹å¾çš„æ•°é‡ã€‚



(2)åŒé‡‡æ ·åµŒå…¥

æœ¬äººçš„å¯å‘æ€æƒ³æ¥è‡ª[2008]EasyEnsembleï¼Œç›®çš„æ˜¯è§£å†³åˆ†ç±»ä¸å‡è¡¡çš„é—®é¢˜ã€‚ä¼ªä»£ç å¦‚ä¸‹ï¼š

![2fig3precode](/Malicious_URL_9/2fig3precode.png)

ä¸»è¦çš„æ€æƒ³å°±æ˜¯ï¼šä¸»è¦åœ¨æ•°æ®é‡å¤§å¤§æ•°æ®é›†ä¸Šå–æ ·ã€‚æœ¬æ–‡çš„å·®å¼‚æ˜¯ï¼Œä¹Ÿåœ¨è®­ç»ƒé›†ä¸Šå–æ ·ã€‚


7ã€æ•°æ®

é’“é±¼ç½‘ç«™(1683ä¸ª)ï¼šhttps://etherscan.io/accounts/label/phish-hack 

åˆæ³•æ•°æ®ï¼šhttps://www.parity.io/ethereum/



ç”±äºä¸¤è€…æ•°æ®é‡å·®åˆ«è¾ƒå¤§ï¼Œéœ€è¦è¿›è¡Œæ•°æ®æ¸…æ´—ã€‚å°†ä¸€äº›å¾ˆæ˜æ˜¾çš„è‰¯æ€§æ•°æ®åˆ é™¤æ‰ï¼Œä¸è¿›è¡Œè®­ç»ƒã€‚

æœ¬æ–‡1)è¿‡æ»¤æ‰äº†æ™ºèƒ½åˆçº¦çš„åœ°å€ï¼Œ2) è¿‡æ»¤æ‰å°‘äº10ä¸ªæˆ–å¤šäº1000æ¡äº¤æ˜“è®°å½•çš„è´¦æˆ· 3) å¿½è§†æ‰åœ¨block height 2ç™¾ä¸‡ä¹‹å‰çš„è®°å½•ã€‚

åŸå› æ˜¯ï¼š1) æ™ºèƒ½åˆçº¦é€šå¸¸é€»è¾‘å¤æ‚ï¼Œä¸æ˜“æ‰§è¡Œé’“é±¼æ£€æµ‹ï¼Œå¹¶ä¸”æ™ºèƒ½åˆçº¦ä¸­åªæœ‰å¾ˆå°ä¸€éƒ¨åˆ†(2.6%)åœ¨é’“é±¼æ£€æµ‹ä¸­ï¼Œé€šå¸¸ä¸tokensç›¸å…³ã€‚2)  äº¤æ˜“è®°å½•è¿‡å°‘ï¼Œä¸æ˜“è¯„ä»·ï¼Œè¿‡å¤šï¼Œåˆ™è´¦æˆ·å¯èƒ½æ˜¯wallet æˆ–å…¶ä»–è´¦æˆ·ç±»å‹ã€‚äº‹å®ä¸Šï¼Œæœ‰è¶…è¿‡70%çš„è´¦æˆ·éƒ½æœ‰è¶…è¿‡1000æ¡äº¤æ˜“è®°å½•ï¼Œä½†æ˜¯åªæœ‰1ä¸ªæ˜¯è¢«æ ‡è®°ä¸ºé’“é±¼çš„ã€‚ 3) åˆ†æå‘ç°é’“é±¼åœ°å€çš„æ´»åŠ¨æ—¶é—´éƒ½åœ¨2016-08-02ä¹‹åï¼Œå¯èƒ½æ˜¯æ—©æœŸé’“é±¼è´¦æˆ·è¾ƒå°‘ï¼Œè®°å½•å°±æ›´å°‘äº†ã€‚



æœ€ç»ˆé€‰å–äº†534ï¼Œ820ä¸ªåœ°å€ï¼Œå…¶ä¸­323ä¸ªæ˜¯é’“é±¼åœ°å€ã€‚



8ã€å®éªŒç»“æœ

é‡‡ç”¨äº†k-foldäº¤å‰éªŒè¯ï¼Œå…¶ä¸­k = 5

é‡‡ç”¨4ä¸ªæŒ‡æ ‡è¯„ä»·ï¼šprecision / recall / F1 / AUCã€‚



æ¨¡å‹æ¯”è¾ƒï¼šæ¯”è¾ƒlightGBMã€SVMã€DTå’ŒåŒé‡‡æ ·åµŒå…¥æ¨¡å‹(DE+)ã€‚è®¾å®šç‰¹å¾é‡‡æ ·ç‡ä¸º70%ï¼ŒåŸºç¡€æ¨¡å‹çš„æ•°é‡ä¸º1600ï¼Œç»“æœå¦‚ä¸‹ã€‚å…¶ä¸­SVMæ•ˆæœæœ€å·®ã€‚åŠ å…¥DE+ä¹‹åï¼Œè¿™ä¸‰ä¸ªæ¨¡å‹æ•ˆæœéƒ½æå‡äº†ã€‚

![2fig4resultmodel](/Malicious_URL_9/2fig4resultmodel.png)

é‡‡æ ·çš„æœ‰æ•ˆæ€§åˆ†æï¼š
åŸºæ¨¡å‹çš„æ•°é‡å¯¹ç»“æœçš„å½±å“ã€‚

![2fig5](/Malicious_URL_9/2fig5.png)

ç‰¹å¾é‡‡æ ·è¯„ä¼°

è®¾å®šä¸åŒçš„é‡‡æ ·ç‡ï¼Œä¸”åŸºæ¨¡å‹çš„æ•°é‡è®¾ä¸º1600ã€‚

![2fig6](/Malicious_URL_9/2fig6.png)

0.8æ•ˆæœæœ€å¥½ã€‚è¯´æ˜å¹¶ä¸æ˜¯ç‰¹å¾å´å¤šè¶Šå¥½ã€‚å¦‚æœç‰¹å¾å¤ªå¤šï¼Œç‰¹å¾é‡‡æ ·çš„æ–¹æ³•å¯èƒ½ä¼šæé«˜æ€§èƒ½ã€‚è¿™å¯èƒ½æ˜¯å› ä¸º feature sampling can make different base models view the object from different angles,

è¿™ä¸ªå› ç´ æœ‰ä¸€å®šå½±å“ï¼Œä½†å½±å“æ²¡æœ‰ä¸Šä¸€ä¸ªå› ç´ é‚£ä¹ˆå¤§ã€‚



9ã€ç‰¹å¾åˆ†æ

top 15ç‰¹å¾å¦‚ä¸‹ï¼š

![2fig7](/Malicious_URL_9/2fig7.png)

in_block_stdï¼šååº”äº†æŸä¸ªç‰¹å®šåœ°å€çš„inäº¤æ˜“çš„å¼ºåº¦ã€‚é’“é±¼ç½‘ç«™çš„inäº¤æ˜“çªç„¶å¢åŠ ã€‚ç„¶è€Œå½“é’“é±¼ç½‘ç«™è¢«æŠ«éœ²ä¹‹åï¼Œinäº¤æ˜“ä¼šå˜å°‘ç”šè‡³æ¶ˆå¤±ã€‚è¿™å¯¼è‡´é’“é±¼ç½‘ç«™çš„inäº¤æ˜“åŸºæœ¬ä¸Šéƒ½é›†ä¸­åœ¨æŸä¸€ä¸ªæ—¶é—´æ®µå†…ã€‚

to_out_sum_medianï¼šmedianå¯ä»¥çœ‹æˆæŒ‡ä»£ç€ç»æµstrengthï¼Œå¯¹äºé’“é±¼åœ°å€ï¼Œto æœ‹å‹æ˜¯é’“é±¼ç½‘å€çš„å—å®³è€…ï¼Œè¿™ä¸ªç‰¹å¾æŒ‡ä»£çš„æ˜¯å—å®³è€…åŸºç»æµæ°´å¹³ã€‚

from_in_sum_minï¼šé’“é±¼ç½‘ç«™é€šå¸¸éœ€è¦æ´—é’±ï¼Œè¿™ä¸ªç‰¹å¾é€šå¸¸ä½œä¸ºè¿‡æ¸¡çš„åœ°å€ï¼Œè¡Œä¸ºä¸æ­£å¸¸åœ°å€å¾ˆä¸ä¸€æ ·ã€‚



æ”¶è·ï¼š

1ã€ä»£ç å¼€æºï¼--- æ²¡æ‰¾åˆ°ä»£ç ï¼Œå€Ÿé‰´è¿™ç§æ€æƒ³å§ï¼

2ã€é’ˆå¯¹æ¶æ„æ ·æœ¬è¾ƒå°‘çš„é—®é¢˜ï¼Œèƒ½å¦é‡‡ç”¨æœ¬æ–‡çš„åŒé‡‡æ ·æ–¹æ³•ï¼Ÿå¦‚æœä¸ºäº†å¹³è¡¡æ•°æ®ï¼Œå¯ä»¥é€‰æ‹©è‰¯æ€§é‡Œé¢çœ‹èµ·æ¥ä¸é‚£ä¹ˆè‰¯çš„ï¼Œä»¥å¢åŠ æ¨¡å‹çš„è¯†åˆ«èƒ½åŠ›ã€‚

3ã€å¦‚æœç‰¹å¾å¤ªå¤šï¼Œå¯ä»¥é‡‡ç”¨é‡‡æ ·çš„æ–¹æ³•ã€‚è€ƒè™‘èƒ½å¦ç”¨åˆ°n-gramsé‡Œé¢ã€‚


## äºŒåå…­ã€XLNetï¼šè¯­è¨€ç†è§£çš„å¹¿ä¹‰å›å½’é¢„è®­ç»ƒ

1ã€arXivä¸Šçš„æ–‡ç« ï¼Œä½œè€…æ¥è‡ªå¡å†…åŸºæ¢…éš†å¤§å­¦ï¼Œè°·æ­Œäººå·¥æ™ºèƒ½å¤§è„‘å›¢é˜Ÿã€‚



2ã€é¢„è®­ç»ƒ

ARï¼šè‡ªå›å½’ï¼Œç”¨ä¸€ä¸ªå›å½’æ¨¡å‹æ¥ä¼°è®¡æ–‡æœ¬è¯­æ–™åº“çš„æ¦‚ç‡åˆ†å¸ƒã€‚å…¶åªè¢«è®­ç»ƒä¸ºç¼–ç å•å‘ä¸Šä¸‹æ–‡ï¼ˆå‘å‰æˆ–å‘åï¼‰

AEï¼šè‡ªåŠ¨ç¼–ç ï¼Œä»æŸåçš„è¾“å…¥é‡å»ºåŸå§‹æ•°æ®ã€‚ä¾‹å¦‚BERTã€‚å¯ä»¥åŒå‘ã€‚

æœ¬æ–‡ç”¨åˆ°äº†ARå’ŒAEçš„ä¼˜ç‚¹ã€‚



3ã€æœ¬æ–‡æ–¹æ³•ï¼š

pretraining objectiveï¼š

(1)ARä¸­ï¼Œæ²¡æœ‰ä½¿ç”¨å›ºå®šçš„å› å¼åˆ†è§£é˜¶ï¼Œè€Œæ˜¯æœ€å¤§åŒ–ä¸€ä¸ªåºåˆ—çš„æœŸæœ›å¯¹æ•°ä¼¼ç„¶ï¼Œä¹Ÿå°±æ˜¯ æ‰€æœ‰å¯èƒ½çš„å› å­åˆ†è§£é¡ºåºæ’åˆ—ã€‚

(2)ä¸”æœ¬æ–‡çš„XLNetä¸ä¾èµ–äºæ•°æ®æŸåã€‚åŒæ—¶ï¼Œè‡ªå›å½’ç›®æ ‡è¿˜æä¾›äº†ä¸€ç§è‡ªç„¶çš„æ–¹æ³• æ¥ä½¿ç”¨ä¹˜ç§¯è§„åˆ™åˆ†è§£é¢„æµ‹ä»¤ç‰Œçš„è”åˆæ¦‚ç‡ï¼Œæ¶ˆé™¤äº†BERTä¸­çš„ç‹¬ç«‹æ€§å‡è®¾ã€‚

architectural designs for pretrainingï¼š

(1) XLNetå°†Transformer XLçš„æ®µé€’å½’æœºåˆ¶å’Œç›¸å…³ç¼–ç æ–¹æ¡ˆ[9]é›†æˆåˆ°é¢„è®­ç»ƒä¸­ã€‚

(2) å•çº¯åœ°å°†Transformerï¼ˆ-XLï¼‰ä½“ç³»ç»“æ„åº”ç”¨äºåŸºäºç½®æ¢çš„è¯­è¨€å»ºæ¨¡æ˜¯è¡Œä¸é€šçš„ï¼Œå› ä¸ºåˆ†è§£é¡ºåºæ˜¯ä»»æ„çš„ï¼Œç›®æ ‡æ˜¯ä¸æ˜ç¡®çš„ã€‚è§£å†³æ–¹æ¡ˆæ˜¯ï¼šé‡æ–°ä¸ºTransformerè®¾å®šå‚æ•°ã€‚



æ”¶è·ï¼š

1ã€èƒ½å¦ç†è§£URLçš„å«ä¹‰ï¼Ÿ

2ã€ä¹‹å‰æ²¡äº†è§£è¿‡nlpï¼Œè¿™ç¯‡æ–‡ç« åŸºæœ¬éƒ½çœ‹ä¸æ‡‚ï¼Œä»£ç å¼€æºï¼Œä½†æ˜¯ä¹Ÿæ²¡å»çœ‹ã€‚æ„Ÿè§‰æ–¹å‘è¿˜ä¸å¤ªé‡åˆï¼Œæ²¡æœ‰æ‰¾åˆ°å¤ªå¤šå¯ä»¥å€Ÿé‰´çš„æ€æƒ³ã€‚



## äºŒåä¸ƒã€Universal Sentence Encoder é€šç”¨å¥å­ç¼–ç å™¨

1ã€arXivä¸Šçš„æ–‡ç« ï¼Œä½œè€…æ¥è‡ªè°·æ­Œã€‚



2ã€å¯ç”¨çš„æ•°æ®é›†ä¸å¤Ÿï¼Œè®¸å¤šæ¨¡å‹é€šè¿‡ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„å•è¯åµŒå…¥ï¼ˆå¦‚word2vecï¼ˆMikolov et al.ï¼Œ2013ï¼‰æˆ–GloVeï¼ˆPennington et al.ï¼Œ2014ï¼‰ç”Ÿæˆçš„å•è¯åµŒå…¥ï¼Œé€šè¿‡éšå¼å½¢æˆæœ‰é™çš„è¿ç§»å­¦ä¹ æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚



3ã€æ–‡ç« ä¸­è¿˜æœ‰è¾ƒå¤šå†…å®¹ï¼Œä½†æ˜¯å…ˆä¸çœ‹äº†ï¼Œé‡ç‚¹çœ‹ä¸€ä¸‹æ¨¡å‹æ€ä¹ˆä½¿ç”¨



ä»£ç å®ç°

å‚è€ƒhttps://colab.research.google.com/github/tensorflow/hub/blob/50bbebaa248cff13e82ddf0268ed1b149ef478f2/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb#scrollTo=MSeY-MUQo2Ha

ipythonä¸­

```python
#å¯¼å…¥åº“
from absl import logging

#import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior( )
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns

#è½½å…¥æ¨¡å‹
module_url = "https://tfhub.dev/google/universal-sentence-encoder/4"
model = hub.load(module_url)
print ("module %s loaded" % module_url)
def embed(input):
  return model(input)
```

urlè½½å…¥æ¨¡å‹å¤±è´¥ï¼Œæ€»æ˜¾ç¤ºæ—¶é—´è¶…æ—¶ã€‚

æƒ³è¦ä¸‹è½½.pbæ¨¡å‹ä»¥åŠå˜é‡æ–‡ä»¶ç„¶åå¯¼å…¥ï¼Œä½†æ˜¯è¿˜æ˜¯æœ‰ç‚¹é—®é¢˜å†åŠ ä¸ŠæœåŠ¡å™¨ä¼ ä¸äº†å¾ˆå¤§çš„æ–‡ä»¶ã€‚

```python
from tensorflow.python.platform import gfile
 
#sess = tf.Session()
sess = tf.compat.v1.InteractiveSession()
with gfile.FastGFile('saved_model.pb', 'rb') as f:
    graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())
    graph_def.ParseFromString(f.read())
    sess.graph.as_default()
    tf.import_graph_def(graph_def, name='')

```



åœ¨colabä¸­è¿è¡Œå§ï½

ä»¥ä¸‹ä¸»è¦ä½“ç°çš„æ˜¯ä¸ç®¡é•¿å¥å­è¿˜æ˜¯çŸ­å¥å­éƒ½å¯ä»¥ï½

```python
#@title Compute a representation for each message, showing various lengths supported.
word = "Elephant"
sentence = "I am a sentence for which I would like to get its embedding."
paragraph = (
    "Universal Sentence Encoder embeddings also support short paragraphs. "
    "There is no hard limit on how long the paragraph is. Roughly, the longer "
    "the more 'diluted' the embedding will be.")
messages = [word, sentence, paragraph]

# Reduce logging output.
logging.set_verbosity(logging.ERROR)

message_embeddings = embed(messages)

#message_embeddingsç»“æœå¦‚ä¸‹
tf.Tensor(
[[ 0.00834449  0.00048082  0.06595246 ... -0.03266348  0.02640911
  -0.0606688 ]
 [ 0.05080861 -0.01652431  0.01573779 ...  0.00976659  0.03170122
   0.01788117]
 [-0.0283327  -0.05586218 -0.01294146 ... -0.05133027  0.01178872
   0.00579202]], shape=(3, 512), dtype=float32)


for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):
  print("Message: {}".format(messages[i]))
  print("Embedding size: {}".format(len(message_embedding)))
  message_embedding_snippet = ", ".join(
      (str(x) for x in message_embedding[:3]))
  print("Embedding: [{}, ...]\n".format(message_embedding_snippet))
```

ç»“æœå¦‚ä¸‹

```python
Message: Elephant
Embedding size: 512
Embedding: [0.008344488218426704, 0.00048081763088703156, 0.06595246493816376, ...]

Message: I am a sentence for which I would like to get its embedding.
Embedding size: 512
Embedding: [0.0508086122572422, -0.016524313017725945, 0.015737786889076233, ...]

Message: Universal Sentence Encoder embeddings also support short paragraphs. There is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.
Embedding size: 512
Embedding: [-0.02833268605172634, -0.055862169712781906, -0.012941470369696617, ...]
```

ä»¥ä¸‹å¯ä»¥è®¡ç®—ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼åº¦

```python
def plot_similarity(labels, features, rotation):
  corr = np.inner(features, features)	#è®¡ç®—å†…ç§¯ï¼Œä»¥è¡Œä¸ºå•ä½æ¥è¿›è¡Œè®¡ç®—çš„
  sns.set(font_scale=1.2)
  g = sns.heatmap(
      corr,
      xticklabels=labels,
      yticklabels=labels,
      vmin=0,	#è®¾å®šæœ€å°å€¼å’Œæœ€å¤§å€¼
      vmax=1,	
      cmap="YlOrRd")
  g.set_xticklabels(labels, rotation=rotation)
  g.set_title("Semantic Textual Similarity")

def run_and_plot(messages_):
  message_embeddings_ = embed(messages_)	#è¿›è¡ŒåµŒå…¥
  plot_similarity(messages_, message_embeddings_, 90)	#ç”»å›¾
```

å…¶å®æœ¬è´¨ä¸Šå°±æ˜¯æŠŠåŸæ¥çš„å¼å­è®¡ç®—å†…ç§¯ï¼Œç›¸åº”çš„ä½ç½®å†…ç§¯çš„å€¼è¶Šå¤§ï¼Œè¯´æ˜ç›¸å…³æ€§è¶Šå¼º



semantic textual ç›¸ä¼¼æ€§(STS) åŸºå‡†è¯„ä¼°

```python

#sts_devå¦‚ä¸‹ï¼š
sts_data :         sim  ...                                             sent_2
0     5.00  ...               A man wearing a hard hat is dancing.
1     4.75  ...                         A child is riding a horse.
2     5.00  ...           The man is feeding a mouse to the snake.
3     2.40  ...                           A man is playing guitar.
4     2.75  ...                          A man is playing a flute.
...    ...  ...                                                ...
1465  2.00  ...                 Has Nasa discovered water on Mars?
1466  0.00  ...     WTO: India regrets action of developed nations
1467  2.00  ...  Volkswagen's "gesture of goodwill" to diesel o...'
1468  0.00  ...  Obama waiting for midterm to name attorney gen...
1469  0.00  ...  New York police officer critically wounded in ...

[1468 rows x 3 columns]

#dev_scoreså¦‚ä¸‹ï¼š
dev_scores: [5.0, 4.75, 5.0, 2.4, 2.75, 2.615, 5.0, 2.333, 3.75, 5.0, 3.2, 1.5830000000000002, 5.0, 5.0, 4.909, 0.8, 2.4, 5.0, 4.0, 0.636, 3.0, 1.714, 3.2, 2.167, 1.0, 1.9169999999999998, 4.25, 3.0, 1.0, 0.6, 2.6, 5.0, 4.6, 5.0, 4.8, 3.8, 5.0, 5.0, 4.2, 1.4, 3.6, 2.8, 1.6, 3.0, 1.4, 0.25, 0.25, 0.0, 4.0, 4.5, 0.5, 3.8, 4.8, 5.0, 0.25, 1.2, 0.6, 0.8, 3.8, 0.0, 3.5, 4.5, 2.8, 3.8, 3.8, 0.0, 4.0, 4.25, 2.812, 4.25, 3.0, 1.0, 3.75, 0.0, 0.4, 4.0, 2.6, 0.8, 2.4, 1.0, 0.2, 0.6, 3.6, 0.2, 0.2, 0.0, 0.0, 1.2, 0.0, 0.4, 0.4, 3.0, 4.4, 3.2, 3.4, 3.4, 3.6, 0.8, 0.8, 1.0, 1.6, 1.0, 3.2, 2.0, 0.0, 0.6, 0.0, 1.4, 2.0, 2.6, 4.2, 2.8, 0.0, 0.0.6, 0.0, 0.0, 0.0, 0.0, 2.2, 0.0, 0.4, 0.0, 2.6, 0.8, 0.4, 3.0, 2.2, 0.6, 3.4, 2.2, 1.8, 0.8, 
             ......
4.6, 3.8, 1.4, 3.8, 0.0, 1.6, 5.0, 0.4, 2.4, 3.2, 3.8, 2.6, 1.0, 4.0, 2.0, 2.2, 4.6, 4.4, 1.8, 0.0, 0.0, 2.0, 3.4, 0.2, 2.4, 0.0, 0.4, 1.8, 0.8, 0.4, 2.2, 1.8, 4.6, 2.2, 1.0, 1.0, 4.2, 0.4, 0.2, 0.0, 0.0, 0.0, 0.6, 0.6, 4.0, 0.0, 0.8, 3.2, 4.8, 4.2, 2.2, 1.0, 3.6, 1.6, 3.0, 2.4, 0.4, 0.0, 5.0, 1.2, 1.4, 4.0, 4.6, 3.4, 0.4, 0.0, 4.8, 5.0, 4.0, 4.0, 3.0, 5.0, 2.0, 3.0, 3.0, 1.0, 5.0, 2.0, 2.0, 2.0, 5.0, 3.0, 3.0, 0.0, 2.0, 0.0, 2.0, 0.0, 0.0]
Pearson correlation coefficient = 0.8036389313002914
p-value = 0.0
```



```python
sts_data = sts_dev #@param ["sts_dev", "sts_test"] {type:"raw"}

def run_sts_benchmark(batch):
  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_1'].tolist())), axis=1)
  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)
  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)
  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)
  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi
  """Returns the similarity scores"""
  return scores

dev_scores = sts_data['sim'].tolist()
scores = []
for batch in np.array_split(sts_data, 10):	#åˆ†æˆ10ä»½
  scores.extend(run_sts_benchmark(batch))

pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)
print('Pearson correlation coefficient = {0}\np-value = {1}'.format(
    pearson_correlation[0], pearson_correlation[1]))
```


è‡ªå·±å°è¯•ç”¨3ä¸ªURLå»ç¼–ç ï¼Œç»“æœå¦‚ä¸‹

![3fig1](/Malicious_URL_9/3fig1.png)

å¦‚æœå¯¹URLè¿›è¡Œä»¥ä¸‹é¢„å¤„ç†ï¼Œ.ã€/ã€ç­‰å·ã€å˜ç©ºæ ¼ï¼Œ-å’Œ_ç›´æ¥åˆ é™¤

è‰¯æ€§çš„ç»“æœå¥½å¾ˆå¤šï¼Œæ¶æ„æœ‰ç‚¹ç¦»è°±ï½

![3fig2](/Malicious_URL_9/3fig2.png)



## äºŒåå…«ã€é€šè¿‡ç½‘é¡µä¸­çš„ 6 ä¸ªç‰¹å¾å­—æ®µæ£€æµ‹é’“é±¼ç½‘ç«™

å‘è¡¨äºå˜¶å¼çš„å…¬ä¼—å·ï¼Œé“¾æ¥https://mp.weixin.qq.com/s/qQUQLlrALkBz-adWz56BTg

1ã€æ£€æµ‹ç½‘é¡µçš„HTMLç‰‡æ®µã€‚

æ”»å‡»è€…å¤åˆ¶ç½‘ç«™çš„æ—¶å€™ï¼Œå¯èƒ½å¤åˆ¶äº†ä¸€äº›æ— ç”¨çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯æ£€æµ‹å‡ºè¿™äº›æ— æ„ä¹‰ã€è¢«å¤åˆ¶è¿›å»äº†çš„ä¿¡æ¯ã€‚

è¿™äº›æ— æ„ä¹‰çš„æ–‡ä»¶å¯èƒ½æ˜¯å“ˆå¸Œã€ç‰ˆæœ¬æ§åˆ¶ã€SaaSçš„APIå¯†é’¥ã€CSRFä»¤ç‰Œã€å†…å®¹å®‰å…¨ç­–ç•¥(CSP)çš„éšæœºæ•°ã€å­èµ„æºå®Œæ•´æ€§å“ˆå¸Œã€‚



è¿™ç§æƒ³æ³•æŒºå·§å¦™çš„ï¼



## äºŒåä¹ã€ä½¿ç”¨è¯æ±‡ç‰¹å¾æ£€æµ‹é’“é±¼ç½‘ç«™

1ã€æ–‡ç« å‘è¡¨äºComputer Communicationsï¼Œ2021ã€‚



2ã€é€‰å–9ä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼Œä½¿ç”¨ç®—æ³•Spearman correlation, K best, and Random forestè®¡ç®—ç‰¹å¾æƒé‡ï¼Œç„¶åè¾“å…¥åˆ°æœºå™¨å­¦ä¹ ç®—æ³•ä¸­ã€‚

![4fig1](/Malicious_URL_9/4fig1.png)

ç‰¹å¾å¦‚ä¸‹ï¼š

(1) åŸŸä¸­çš„tokenæ•°

ç™¾åº¦å‘ç°è¿™ä¸ªå¯ä»¥ç›´æ¥ç”¨parseæ¥å®ç°

```python
from urllib.parse import urlparse
url = 'http://netloc/path;param?query=arg#frag'
parsed = urlparse(url)
print(parsed)
print(len(parsed))
```

ç»“æœå¦‚ä¸‹

```python
url = 'http://netloc/path;param?query=arg#frag'

ParseResult(scheme='http', netloc='netloc', path='/path', params='param', query='query=arg', fragment='frag')

In [20]: len(parsed)
Out[20]: 6
```



(2) é¡¶çº§åŸŸåçš„æ•°é‡ TLD

è‡ªå·±ä¿å­˜ä¸€ä¸ªé¡¶çº§åŸŸååˆ—è¡¨ï¼Œå¦‚æœåœ¨è¿™ä¸ªåˆ—è¡¨ä¸­çš„æ‰ç®—åšæ˜¯é¡¶çº§åŸŸåã€‚

åˆ«çš„ç¬¬ä¸‰æ–¹åº“å¥½åƒä¸èƒ½ç”¨ï¼Œå› ä¸ºä»–ä»¬åªèƒ½æå–å‡ºä¸€ä¸ªé¡¶çº§åŸŸï¼Œä¸èƒ½æå–å‡ºå¤šä¸ªã€‚åªèƒ½è‡ªå·±å†™ä¸ªæ­£åˆ™è¡¨è¾¾å¼çš„å­—ç¬¦ä¸²åŒ¹é…ã€‚

å®ç°æ€è·¯ï¼š

```python
.å‡ºç°
åŒ¹é…åé¢çš„å­—æ¯
åœ¨åº“ä¸­æœç´¢
```

...

```python
from urllib.parse import urlparse
import re


print("--"*40)
for url in urls:
  print
    parts = urlparse(url)
    host = parts.netloc
    m = pattern.search(host)
    res =  m.group() if m else host
    print("unkonw") if not res else res

```

åº“

```python
[a-z0-9-]{1,63}(.ab.ca|.bc.ca|.mb.ca|.nb.ca|.nf.ca|.nl.ca|.ns.ca|.nt.ca|.nu.ca|.on.ca|.pe.ca|.qc.ca|.sk.ca|.yk.ca|.co.cc|.com.cd|.net.cd|.org.cd|.co.ck|.ac.cn|.com.cn|.edu.cn|.gov.cn|.net.cn|.org.cn|.ah.cn|.bj.cn|.cq.cn|.fj.cn|.gd.cn|.gs.cn|.gz.cn|.gx.cn|.ha.cn|.hb.cn|.he.cn|.hi.cn|.hl.cn|.hn.cn|.jl.cn|.js.cn|.jx.cn|.ln.cn|.nm.cn|.nx.cn|.qh.cn|.sc.cn|.sd.cn|.sh.cn|.sn.cn|.sx.cn|.tj.cn|.xj.cn|.xz.cn|.yn.cn|.zj.cn|.us.com|.com.cu|.edu.cu|.org.cu|.net.cu|.gov.cu|.inf.cu|.gov.cx|.com.dz|.org.dz|.net.dz|.gov.dz|.edu.dz|.asso.dz|.pol.dz|.art.dz|.com.ec|.info.ec|.net.ec|
...                .win|.work|.wtf|.xxx|.XYZ|.kaufen|.desi|.shiksha|.moda|.futbol|.juegos|.uno|.africa|.asia|.krd|.taipei|.tokyo|.alsace|.amsterdam|.bcn|.barcelona|.berlin|.brussels|.bzh|.cat|.cymru|.eus|.frl|.gal|.gent|.irish|.istanbul|.istanbul|.london|.paris|.saarland|.scot|.swiss|.wales|.wien|.miami|.nyc|.quebec|.vegas|.kiwi|.melbourne|.sydney|.lat|.rio|.ru|.aaa|.abb|.aeg|.afl|.aig|.airtel|.bbc|.bentley|.example|.invalid|.local|.localhost|.onion|.testa)$
```



è¿™ä¸ªæ²¡å†™å‡ºæ¥ï¼Œåœ¨å±±ç©·æ°´å°½çš„æ—¶å€™å‘ç°githubä¸Šæœ‰è‡ªåŠ¨æå–ç‰¹å¾çš„ä»£ç ï¼æˆ‘åˆæ´»äº†ï½

```python
import re
def count_tld(text):
    """Return amount of Top-Level Domains (TLD) present in the URL."""
    file = open('tlds.txt', 'r')
    count = 0
    pattern = re.compile("[a-zA-Z0-9.]")
    for line in file:
        i = (text.lower().strip()).find(line.strip())
        while i > -1:
            if ((i + len(line) - 1) >= len(text)) or not pattern.match(text[i + len(line) - 1]):
                count += 1
            i = text.find(line.strip(), i + 1)
    file.close()
    return count

url = "https://blog.csdn.net/weixin_42793426/article/details.xyz.cn/88545939.xyz"
count_tld(url)
```

å¤§è‡´æ€è·¯å°±æ˜¯ä»åº“ä¸­ä¸€ä¸ªä¸€ä¸ªå»æ¯”å¯¹ï¼Œçœ‹çœ‹æœ‰æ²¡æœ‰å‡ºç°è¿‡ï½å¤ªå·§å¦™äº†ï¼



(3) URLçš„é•¿åº¦

```python
def  length_url(text):
        return len(text)
```



(4) æŸ¥è¯¢ä¸­çš„æ•°æ®

```python
from urllib.parse import urlparse
import re

def count_digit_in_query(text):
    content_query = urlparse(text).query
    print(content_query)
    count = 0
    return len(re.findall(r'=[1-9][^a-z]', content_query))

url = "https://blog.csdn.net/weixin_42793426/article?id=4/details.xyz.cn/88545939.xyz?id=3cdjdvjdfbv/csjhvufhgui?id=11"
res = count_digit_in_query(url)
if(res == 0): res = -1
```

è¿™ä¸ªä¹Ÿæ˜¯è‡ªå·±å†™çš„ï¼Œç›¸å½“äºåœ¨queryä¸­åŒ¹é… `=æ•°å­—`ï¼Œæœ‰ä¸€ä¸ªç¼ºé™·å°±æ˜¯?id=34bjsnfejrjvä¹Ÿä¼šè¢«åŒ¹é…åˆ°ï¼Œè®¤ä¸ºæ˜¯æ•°å­—æŸ¥è¯¢ã€‚ä¸è¿‡è¿™ç§æƒ…å†µåº”è¯¥å¾ˆå°‘ï¼Œå°±å…ˆä¸ç®¡ä»–å•¦ï½



(5) URLä¸­ç‚¹çš„æ•°é‡

```python
def count(text, character):
    """Return the amount of certain character in the text."""
    return text.count(character)

url = "http://127.0.0.1:800....0"
res = count(url, ".")
```



(6) åŸŸä¸­å®šç•Œç¬¦çš„æ•°é‡

å› ä¸ºåœ¨åŸŸä¸­å¯èƒ½ä¼šåšä¸€äº›æ··æ·†ï¼Œä½¿å¾—ä¸€ä¸ªURLçœ‹èµ·æ¥åƒæ˜¯åˆæ³•çš„URLã€‚

å…ˆæ‰¾åˆ°æ‰€æœ‰çš„åŸŸï¼Œå†åˆ†åˆ«æ‰¾åˆ°æ¯ä¸ªåŸŸä¸­çš„å®šç•Œç¬¦ã€‚ä½†æ˜¯æ‰¾åˆ°æ‰€æœ‰çš„åŸŸæœ‰ç‚¹éš¾åº¦ï¼Œç›´æ¥ç”¨urlparseçš„netlocã€‚

```python
from urllib.parse import urlparse
def count(text, character):
    """Return the amount of certain character in the text."""
    return text.count(character)
url = "https://bl-og.csdn.net/weixin_42793426/article?id=4/details.xyz.cn/88545939.xyz?id=3cdjdvjdfbv/csjhvufhgui?id=11"
domain = urlparse(url).netloc
count_res = 0;
delimiter_list = ['.','-','_','/','?','=','@','&','!',' ','~',',','+','*','#','$','%',';','{','|','(']
for symbol in delimiter_list:
  count_res += count(domain,symbol)
```

å¦‚æœåç»­æ”¹è¿›çš„è¯ï¼Œå€ŸåŠ©count_tldã€‚if count_tld > 1ï¼Œåé¢å†æœç´¢ä¸€æ¬¡



(7) pathä¸­å®šç•Œç¬¦çš„æ•°é‡

```python
from urllib.parse import urlparse
def count(text, character):
    """Return the amount of certain character in the text."""
    return text.count(character)
url = "https://bl-og.csdn.net/weixin_42793426/article?id=4/details.xyz.cn/88545939.xyz?id=3cdjdvjdfbv/csjhvufhgui?id=11"
path = urlparse(url).path
count_res = 0;
delimiter_list = ['.','-','_','/','?','=','@','&','!',' ','~',',','+','*','#','$','%',';','{','|','(']
for symbol in delimiter_list:
  count_res += count(path,symbol)
```



(8) å¤šä¸ªpathä¸­ æœ€é•¿çš„pathçš„æ•°é‡

åŸæœ¬åº”è¯¥æ‰¾å‡ºæ¥å¤šä¸ªpathï¼Œä½†æ˜¯ï¼Œè¿˜æ˜¯ä¸å¤ªå¥½æ‰¾ï¼Œè¿˜æ˜¯ç›´æ¥è®¡ç®—pathçš„é•¿åº¦å§ï½

```python
from urllib.parse import urlparse
url = 'http://netloc/path;param?query=arg#frag'
path = urlparse(url).path
print(path)
print(len(path))
```



(9) åŸŸåé•¿åº¦

```python
from urllib.parse import urlparse
url = "https://bl-og.csdn.net/weixin_42793426/article?id=4/details.xyz.cn/88545939.xyz?id=3cdjdvjdfbv/csjhvufhgui?id=11"
domain = urlparse(url).netloc
print(domain)
print(len(domain))
```



3ã€ç‰¹å¾é‡è¦æ€§

ä½¿ç”¨äº†ä¸‰ç§ç®—æ³•ï¼Œå¦‚ç‰¹å¾ç›¸å…³ã€K-bestå’Œéšæœºæ£®æ—ï¼Œæ¥ç¡®å®šç‰¹å¾çš„é‡è¦æ€§ã€‚

ç‰¹å¾ç›¸å…³ï¼šSpearmanå’ŒPearsonç®—æ³•ã€‚1è´¡çŒ®æ€§å¾ˆå¤§ã€‚

å¯ä»¥å¾—å‡ºç»“è®ºï¼Œè·¯å¾„ä¸­çš„åˆ†éš”ç¬¦ã€åŸŸä¸­çš„åˆ†éš”ç¬¦ã€URLä¸­çš„ç‚¹æ•°ã€URLçš„é•¿åº¦æ˜¯å…¶ä¸­æœ€é‡è¦çš„ç‰¹å¾ã€‚



4ã€ç‰¹å¾é¢„å¤„ç†

åˆ é™¤æ— æ•ˆçš„å­—ç¬¦/æ•°æ®å€¼è¿›è¡Œç¼©æ”¾/çƒ­ç¼–ç /



5ã€æœºå™¨å­¦ä¹ ç®—æ³•ï¼šéšæœºæ£®æ—ã€KNNã€LRã€SVMã€‚

å¯¹å„ä¸ªåˆ†ç±»å™¨èµ‹äºˆæƒå€¼ï¼ŒæŠ•ç¥¨å¾—å‡ºæœ€ç»ˆç»“æœã€‚



ä½†æ˜¯ç›®å‰åˆ°ç°åœ¨ï¼Œè¿˜æ˜¯ä¸çŸ¥é“å…·ä½“çš„è¦å»å¦‚ä½•å®ç°ã€‚

ç”¨è‡ªå·±çš„æ•°æ®åº“è¿è¡Œä¸€ä¸‹GitHubä¸Šçš„ä¸€ä¸ªä¾‹å­ï½

```python
import os
import sys
import re
import matplotlib
import pandas as pd
import numpy as np
from os.path import splitext
import ipaddress as ip
import tldextract
import whois
import datetime
from urllib.parse import urlparse

df = pd.read_csv("10data.csv")
df = df.sample(frac=1).reset_index(drop=True)	#å–æ ·
df.head()


#2016's top most suspicious TLD and words
Suspicious_TLD=['zip','cricket','link','work','party','gq','kim','country','science','tk']
Suspicious_Domain=['luckytime.co.kr','mattfoll.eu.interia.pl','trafficholder.com','dl.baixaki.com.br','bembed.redtube.comr','tags.expo9.exponential.com','deepspacer.com','funad.co.kr','trafficconverter.biz']


# Method to count number of dots
def countdots(url):  
    return url.count('.')

# Method to count number of delimeters
def countdelim(url):
    count = 0
    delim=[';','_','?','=','&']
    for each in url:
        if each in delim:
            count = count + 1
    return count

# Is IP addr present as th hostname, let's validate
import ipaddress as ip #works only in python 3

def isip(uri):
    try:
        if ip.ip_address(uri):
            return 1
    except:
        return 0

#method to check the presence of hyphens
def isPresentHyphen(url):
    return url.count('-')

def isPresentAt(url):
    return url.count('@')
  
def isPresentDSlash(url):
    return url.count('//')
  
def countSubDir(url):
    return url.count('/')
  
def get_ext(url):
    """Return the filename extension from url, or ''."""
    root, ext = splitext(url)
    return ext

def countSubDomain(subdomain):
    if not subdomain:
        return 0
    else:
        return len(subdomain.split('.'))

def countQueries(query):
    if not query:
        return 0
    else:
        return len(query.split('&'))

#ç‰¹å¾é›†è®¾ç½®
featureSet = pd.DataFrame(columns=('url','no of dots','presence of hyphen','len of url','presence of at',\
'presence of double slash','no of subdir','no of subdomain','len of domain','no of queries','is IP','presence of Suspicious_TLD',\
'presence of suspicious domain','label'))

#XXX

for i in range(len(df.head(20000))):
    features = getFeatures(df["url"].loc[i], df["label"].loc[i])    
    featureSet.loc[i] = features
    print(i)
#ä¸Šé¢è¿™ä¸€æ­¥æ˜¯ç‰¹å¾æå–ï¼Œå¯¹äº2ä¸‡æ¡URLï¼Œåœ¨cronlabä¸­å¤„ç†éœ€è¦3åˆ†20ç§’
featureSet.head()

```



ç‰¹å¾çš„çŸ©é˜µä¿¡æ¯å¦‚ä¸‹ï¼Œæ„Ÿè§‰æå–çš„æœ‰ç‚¹é—®é¢˜ã€‚

![4fig2](/Malicious_URL_9/4fig2.png)

ä¸‹é¢å°±æ˜¯æµ‹è¯•çš„æ­¥éª¤äº†ï½



å‡†ç¡®åº¦è¯´å®è¯è¿˜æ˜¯æœ‰ç‚¹ä½äº†ï¼Œè€Œä¸”å‡é˜³ç‡è¿‡é«˜ï¼Œåˆ°70%å·¦å³äº†ï¼Œå¯èƒ½æ˜¯å…¶ä¸­çš„æ­¥éª¤æœ‰é—®é¢˜äº†ã€‚

![4fig3](/Malicious_URL_9/4fig3.png)



æ£€æŸ¥å‘ç°å¯èƒ½æ˜¯ç‰¹å¾æå–æœ‰é—®é¢˜ã€‚å¦‚ä¸‹å›¾ï¼š

![4fig4](/Malicious_URL_9/4fig4.png)

å’‹å¯èƒ½ç‚¹æ•°è¿™ä¹ˆå°‘å‘¢ï½æ£€æŸ¥äº†ä¸€ä¸‹ï¼Œæ„Ÿè§‰æ•°æ®é›†ä¸æ˜¯å¾ˆå¥½ï¼Œæ²¡æœ‰å‰é¢çš„http://



ä½¿ç”¨æ¡Œé¢ä¸Šçš„url data.csvæ–‡ä»¶è¿›è¡Œæµ‹è¯•ï¼Œç„¶åç¨å¾®æ›´æ”¹äº†ä¸€äº›ç‰¹å¾ï¼Œè®©ç‰¹å¾å˜å¾—åˆç†äº†ä¸€äº›ï½ åŠ å…¥äº†TLD count

ä»¥ä¸‹æ˜¯ç”¨200ä¸ªæ•°æ®å»æµ‹è¯•ã€‚

![4fig5](/Malicious_URL_9/4fig5.png)

![4fig6](/Malicious_URL_9/4fig6.png)

å‡†ç¡®åº¦ä¸€ä¸‹è¾¾åˆ°äº†0.9ï¼ï¼ï¼ä»åŸæ¥çš„0.89åˆ°äº†0.92ï¼è´¨çš„é£è·ƒï½

![4fig7](/Malicious_URL_9/4fig7.png)



![4fig8](/Malicious_URL_9/4fig8.png)

è¿™ç»“æœä¹Ÿå¤ªå¥½äº†å­ï¼



ç„¶åç”¨20000æ¡æ•°æ®å»æµ‹è¯•ï½

![4fig9](/Malicious_URL_9/4fig9.png)

å’‹å·®åˆ«ä¸å¤§å•Š

![4fig10](/Malicious_URL_9/4fig10.png)

![4fig11](/Malicious_URL_9/4fig11.png)

å’‹å’Œé¢„æœŸçš„ä¸å¤ªä¸€æ ·å‘¢ï¼Œè¿™ä¸å¤ªå¥½åŒºåˆ†å•Š...

ç»“æœå¦‚ä¸‹ï¼š

![4fig12](/Malicious_URL_9/4fig12.png)

![4fig13](/Malicious_URL_9/4fig13.png)

å¦‚æœç”¨æ‰€æœ‰çš„42ä¸‡æ¡æ•°æ®å»æµ‹è¯•ï½å¤§æ¦‚è¦2hå·¦å³ï¼Œæ—¶é—´æœ‰ç‚¹ä¹…å“¦ğŸ˜¯

é¢„æµ‹ç»“æœå¯èƒ½ä¸ä¼šæ”¹è¿›å¤ªå¤šå§ï¼Œå¯èƒ½ç¨å¾®æé«˜ä¸€äº›



åç»­æ”¹è¿›çš„æ—¶å€™å¯ä»¥åŠ å…¥å¦‚ä¸‹ç‰¹å¾ï¼š

```python
def count_vowels(text):
    """Return the number of vowels."""
    vowels = ['a', 'e', 'i', 'o', 'u']
    count = 0
    for i in vowels:
        count += text.lower().count(i)
    return count
```



æ”¶è·ï¼š

1ã€åŠ å…¥æ–‡ä¸­ï¼šä¸ºäº†é€ƒé¿é»‘åå•æ£€æµ‹ï¼Œæ”»å‡»è€…å¯èƒ½å¯¹URLè¿›è¡Œå¾®å°çš„ä¿®æ”¹ã€‚

2ã€åŠ å…¥æ–‡ä¸­ï¼š

URLæ ¼å¼

![4fig14](/Malicious_URL_9/4fig14.png)

3ã€æœ¬ç¯‡æ¯”è¾ƒå¥½çš„åœ°æ–¹å¯èƒ½æ˜¯ï¼šå®éªŒç»“æœè¾ƒå¥½ + ä½¿ç”¨äº†ç‰¹å¾è¯„åˆ¤ + å¤šä¸ªTLD

â€‹	å¦‚æœæœ‰å¤šä¸ªTLDï¼Œå¦‚ä½•çœŸå®çš„æ•°é‡ï¼Ÿä¸Šæ–‡ä¸­å·²è§£å†³ã€‚



## ä¸‰åã€ä»å‡æ–°é—»åŸŸä¸­å‘ç°å’Œåº¦é‡æ¶æ„é‡å®šå‘æ´»åŠ¨

å¯¼å¸ˆå‘çš„ã€‚

ç›®æ ‡å¯¹è±¡æ˜¯ï¼šå‡æ–°é—»ç½‘ç«™ï¼Œé‡å®šå‘URLã€‚



æ„Ÿè§‰å’Œè‡ªå·±çš„æ–¹å‘ä¸æ˜¯å¾ˆåŒ¹é…ï¼Œä¸å†ç»§ç»­çœ‹äº†ã€‚



## ä¸‰åä¸€ã€Doc2Vecä»£ç å®ç°

çœ‹å­¦é•¿ä¹‹å‰æ¯”èµ›çš„wpï¼Œå­¦ä¹ åˆ°äº†Word2Vecçš„æ”¹è¿›æ–¹æ³•Doc2Vecï¼Œä¸‹é¢æ¥å®ç°ä¸€ä¸‹ï¼Œçœ‹çœ‹å…¶æ•ˆæœã€‚

ä¹‹å‰æµ‹è¯•çš„æ—¶å€™ä¸€ç›´æœ‰é”™ï¼Œç°åœ¨æ¥å°è¯•ä¸€ä¸‹åˆ«äººçš„Word2Vecçš„å®ç°ã€‚



é“¾æ¥å¦‚ä¸‹ï¼šhttps://github.com/ajinkyabhanudas/ML-in-network-security/blob/master/URL%20Classification%20using%20Word2Vec%20and%20FastText-Copy1.ipynb

```python
import pandas as pd
import keras
import matplotlib.pyplot as plt
import numpy as np
from keras.models import load_model
from urllib.parse import urlparse,urlsplit
import gensim
from gensim.models import Word2Vec, FastText
from multiprocessing.pool import ThreadPool
import tensorflow as tf
tf.get_logger().setLevel('ERROR')

def splitter(url_list):
    new_df=pd.DataFrame(columns=["scheme","netloc","path","query","fragment"])
    for url in url_list:
    #     test = [urlsplit(url).scheme, urlsplit(url).netloc, urlsplit(url).path, urlsplit(url).query, urlsplit(url).fragment]
        split_result=urlsplit(url)
        scheme = split_result.scheme
        netloc = split_result.netloc
        path = split_result.path
        query = split_result.query
        new_df =  new_df.append({'scheme': scheme, 'netloc': netloc, 'path': path, 'query': query}, ignore_index= True)
        new_df.fillna(0,inplace=True)
    return (new_df)

def vectorizer_netloc(new_df):
    
    model1 = Word2Vec.load("w2v_url")
    
    
    splitr1= new_df["netloc"]

    splitr1_1= [str(val).split(".") for val in splitr1.tolist()]
    
    model1.build_vocab(splitr1_1, update=True)
    model1.train(splitr1_1, total_examples=len(splitr1_1), epochs=1)

    
    holder1=[]
    vec_df1= pd.DataFrame()
    val=[0 for i in range(100)]
    for i in range(len(splitr1_1)):
        for j in range(len(splitr1_1[i])):
            val += model1[splitr1_1[i][j]]
        holder1.append(val.tolist())
        val *=0
    vec_df1=vec_df1.append(holder1)   
    
    
    return(vec_df1)


def vectorizer_path(new_df):
    
    model2 = Word2Vec.load("w2v_path")
    
    
    splitr2= new_df["path"]
    


    splitr2_1= [str(val).split("/") for val in splitr2.tolist()]
  

    model2.build_vocab(splitr2_1, update=True)
    model2.train(splitr2_1, total_examples=len(splitr2_1), epochs=1)
    

    holder2=[]
    vec_df2= pd.DataFrame()
    val=[0 for i in range(100)]
    for i in range(len(splitr2_1)):
        for j in range(len(splitr2_1[i])):
            val += model2[splitr2_1[i][j]]
        holder2.append(val.tolist())
        val *=0
    vec_df2=vec_df2.append(holder2) 
    
    
    return(vec_df2)


def vectorizer_query(new_df):
    
    model3 = Word2Vec.load("w2v_query")
    
    splitr3= new_df["query"]

    splitr3_1= [str(val).split("=") for val in splitr3.tolist()]


    model3.build_vocab(splitr3_1, update=True)
    model3.train(splitr3_1, total_examples=len(splitr3_1), epochs=1)

    holder3=[]
    vec_df3= pd.DataFrame()
    val=[0 for i in range(100)]
    for i in range(len(splitr3_1)):
        for j in range(len(splitr3_1[i])):
            val += model3[splitr3_1[i][j]]
        holder3.append(val.tolist())
        val *=0
    vec_df3=vec_df3.append(holder3) 
    
    
    return(vec_df3)

def convert_to_df(new_df):
    pool = ThreadPool(processes=2)
    new_df=new_df
    async_result1 = pool.apply_async(vectorizer_netloc, args=(new_df,))
    async_result2 = pool.apply_async(vectorizer_path, args=(new_df,))
    async_result3 = pool.apply_async(vectorizer_query, args=(new_df,))# tuple of args for foo

    vec_df1 = async_result1.get()
    vec_df2 = async_result2.get()
    vec_df3 = async_result3.get()
    
    tester_df_batch=pd.DataFrame()
    tester_df_batch=pd.concat([vec_df1,vec_df2,vec_df3], axis=1)
    return(tester_df_batch)

  def return_avg_for_updated_url_score (eps,model):
    switch = []
    for key in eps.keys():
        for val in range(len(eps[key])):
            switch+=[(eps[key][val],key,calc_endpoint_score(model.predict(convert_to_df(splitter(eps[key]))).tolist()))]

    y =pd.DataFrame(switch, columns=['url','ip','eps_score'])

    grouped_sum = y.groupby('url').sum()
    grouped_size = y.groupby('url').size()
    # grouped_size['a']


    unique_urls = list(y['url'].unique())
    avg_scores_of_endpoints_hitting_the_same_url =[]
    for url in unique_urls :
        avg_scores_of_endpoints_hitting_the_same_url += [{url:grouped_sum['eps_score'][url]/grouped_size[url]}]

    return(avg_scores_of_endpoints_hitting_the_same_url,unique_urls)
  


def calc_endpoint_score(score):
    day_score = float()
    bad_counter = int()
    for sc in score:

        if sc[0] > 0.5:
            day_score += sc[0]
            bad_counter += 1
        else:
            day_score += sc[0]

    bad_counter
    ep_w = (day_score/len(score))*(1.0-(1-(bad_counter/len(score))))
    return(ep_w)
  
def vector_relatedness(url):
    w_ = urlsplit(url)
    w_netloc = w_.netloc.split('.')
    w_path = w_.path.split('.')
    
    if(w_netloc[0]==''):

        related_word=w_path
        if(related_word[0] == 'www'):
            related_word_finder = related_word[1]

        else:
            related_word_finder = related_word[0]

    elif(w_netloc[0]=='www'):
        related_word_finder = w_netloc[1]

    else:
        related_word_finder = w_netloc[0]
        
        
    FastText_model = FastText.load('ft_.model')
    relatedness_vector = FastText_model.wv.most_similar_cosmul(related_word_finder)
    rv =np.mean([i[1] for i in relatedness_vector])
    return(rv)

  
def url_score_recalculator(val_avg, unique_urls, url_scores):
    val_avg = val_avg
    unique_urls = unique_urls
    url_scores = [url_scores[i][0] for i in range(len(url_scores))]
    avg_list = [float(str(*list(val_avg[i].values()))) for i in range(len(val_avg))]
    
    rv =[vector_relatedness(url) for url in unique_urls]
    m_rv_url_score = np.add(np.multiply(url_scores,0.85),np.multiply(rv,0.1))
    
    m_avg_list = np.multiply(avg_list,0.05)
    
    updated_scores = np.subtract(m_rv_url_score,m_avg_list)
    updated_url_score_dict = [{unique_urls[i]:updated_scores[i]} for i in range(len(unique_urls))]
    return(updated_url_score_dict)

  
eps = {'192.168.6.6.1':["https://www.yagle.com/abcd/?=123","https://www.google.com","www.bitsadmin.com/","https://www.yagle.com/abcd/?=123","https://www.google.com","www.bitsadmin.com/","http://zastapiony.piklamp.nl/military.xbel?face=jlthEvD&oil=70UfcEdppE&similar=0T9GXVd&plane=&never=XzUI&size=h-S&building=3NHL3LH4j&play=Ke9C4&over=&pattern=gKK","https://www.google.com"],
      '0.0.0.1':["https://www.yagle.com/abcd/?=123","https://www.google.com","www.bitsadmin.com/"]}

### on assuming an input of the above format to the code

model = load_model('current.best__std.hdf5')
val_avg, unique_urls = return_avg_for_updated_url_score(eps,model)

final_vec = convert_to_df(splitter(eps['192.168.6.6.1']))##
url_scores = model.predict(convert_to_df(splitter(unique_urls))).tolist()
updated_url_score_dict = url_score_recalculator(val_avg, unique_urls, url_scores)

score =  model.predict(final_vec).tolist()
ep_sc = calc_endpoint_score(score)

print("Endpoint Vectors for URLs of '192.168.6.6.1' (3 x 300) : ",final_vec)
print("\nScores for all unique URLs not bound by the endpoint: \n",url_scores)
print("\nUpdated URL scores to dict: \n",updated_url_score_dict)
print("\nScores for URLs of '192.168.6.6.1' endpoint: \n",score)
print("\nEndpoint ('192.168.6.6.1') score : ",ep_sc)

```



ä»£ç å¦‚ä¸Šï¼Œéœ€è¦è½½å…¥æ¨¡å‹æ‰èƒ½å®éªŒï¼Œå¹¶ä¸”æ¨¡å‹æ¯”è¾ƒå¤§ï¼Œå°±ä¸æµ‹è¯•äº†ã€‚ä»£ç çœ‹æ‡‚äº†ï¼Œè§‰å¾—æ¯”è¾ƒæœ‰æ„ä¹‰çš„éƒ¨åˆ†æ˜¯è½¬æ¢æˆçŸ©é˜µçš„ä¸¤ä¸ªforå¾ªç¯ã€‚è‡ªå·±å¯èƒ½ä¼šå€Ÿé‰´åˆ°ã€‚



----



ä¸‹é¢çœ‹ä¸€ä¸ªword2vec + CNNçš„å®ç°ï¼ŒGitHubåœ°å€å¦‚å³è¾¹ï¼Œhttps://github.com/cwellszhang/DetectMaliciousURLï¼Œå‡†ç¡®åº¦æ®è¯´è¾¾åˆ°äº†96%ã€‚

å‚è€ƒæ–‡ç« ï¼šhttps://blog.csdn.net/u011987514/article/details/71189491

æˆ‘æ„Ÿè§‰å¦‚æœæŠŠè¿™ä¸ªä»£ç çœ‹æ‡‚äº†ï¼Œå…¶ä»–ä¸ç®¡å¤šå¤æ‚çš„ä¹Ÿå°±ä¼šäº†ã€‚è¿™ä¸ªåº”è¯¥æ˜¯è‡ªå·±åˆ°ç›®å‰ä¸ºæ­¢è§è¿‡æœ€å¤æ‚çš„ä»£ç äº†ï½

ç”±äºéœ€è¦ä½¿ç”¨tfçš„ç‰ˆæœ¬æ˜¯1.0ï¼Œæ‰€ä»¥åœ¨æœåŠ¡å™¨ä¸­è¿è¡Œï½

æ­¤æ—¶import pandaså‡ºç°é”™è¯¯ï¼Œå‚è€ƒ https://docs.microsoft.com/en-us/answers/questions/55489/modulenotfounderror-no-module-named-39pandas39-whe.htmlï¼Œä½¿ç”¨è¯­å¥ï¼špython2 -m pip install pandas

è¿˜éœ€è¦å®‰è£…gensimï¼Œ`python2 -m pip install -U gensim`å³å¯ã€‚

è¿˜æœ‰tfï¼Œ`python2 -m pip install tensorflow==1.2.1`

è‡³æ­¤ç¯å¢ƒå®‰è£…å®Œæˆï½



è®¾ç½®å‚æ•°ï¼Œç”¨åˆ°äº†tf.flags.DEFINE_XXXï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ·»åŠ å‘½ä»¤è¡Œçš„å¯é€‰å‚æ•°ã€‚å…·ä½“å‚è€ƒhttps://blog.csdn.net/spring_willow/article/details/80111993



å¯¹äºFLAGS.replicas == Falseçš„æƒ…å†µï¼Œ

```python
if FLAGS.replicas==False:
 timestamp = str(int(time.time())) #æ—¶é—´æˆ³
 out_dir = os.path.abspath(os.path.join(os.path.curdir, "runs", timestamp))#ç”¨æ—¶é—´æˆ³å‘½åä¸€ä¸ªæ–‡ä»¶
 print("Writing to {}\n".format(out_dir))
 if not os.path.exists(out_dir):
    os.makedirs(out_dir)
 # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it
 checkpoint_dir = os.path.abspath(os.path.join(out_dir, "checkpoints"))
 checkpoint_prefix = os.path.join(checkpoint_dir, "model")
 if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
```

å…¶å®å°±æ˜¯åˆ›å»ºç›®å½•ã€‚



å¯¹äºFLAGS.replicas == Trueçš„æƒ…å†µï¼š

```python
 # out_dir = os.path.abspath(os.path.join(os.path.curdir, "runs", "replicas"))
 out_dir = os.path.abspath(os.path.join(os.path.curdir,"runs","outputs"))
 print("Writing to {}\n".format(out_dir))
 if not os.path.exists(out_dir):
    os.makedirs(out_dir)
#åˆ›å»ºout_diræ–‡ä»¶ã€‚
    
    
 checkpoint_dir = os.path.abspath(os.path.join(out_dir, "checkpoints"))
 checkpoint_prefix = os.path.join(checkpoint_dir, "model")
 if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
 summary_dir= os.path.abspath(os.path.join(out_dir, "summary"))
 if not os.path.exists(summary_dir):
        os.makedirs(summary_dir)
 train_summary_dir = os.path.join(summary_dir,"train")
 dev_summary_dir= os.path.join(summary_dir,"dev")
 if not os.path.exists(train_summary_dir):
        os.makedirs(train_summary_dir)
 if not os.path.exists(dev_summary_dir):
        os.makedirs(dev_summary_dir)
```

ä¹Ÿæ˜¯åˆ›å»ºç›®å½•


æ¥ä¸‹æ¥å®šä¹‰äº†æ•°æ®é¢„å¤„ç†

```python
def data_preprocess():
    # Data preprocess
    # =======================================================
    # Load data
    print("Loading data...")
    if not os.path.exists(os.path.join(out_dir,"data_x.npy")):
          x, y = data_helper.load_data_and_labels(FLAGS.data_file)
          # Get embedding vector
          x =x[:1000]
          y =y[:1000]
          sentences, max_document_length = data_helper.padding_sentences(x, '<PADDING>',padding_sentence_length=FLAGS.sequence_length)
          print(len(sentences[0]))
          if not os.path.exists(os.path.join(out_dir,"trained_word2vec.model")):
              x= np.array(word2vec_helpers.embedding_sentences(sentences, embedding_size = FLAGS.embedding_dim, file_to_save = os.path.join(out_dir, 'trained_word2vec.model')))
          else:
              print('w2v model found...')
              x = np.array(word2vec_helpers.embedding_sentences(sentences, embedding_size = FLAGS.embedding_dim, file_to_save = os.path.join(out_dir, 'trained_word2vec.model'),file_to_load=os.path.join(out_dir, 'trained_word2vec.model')))
          y = np.array(y)
          # np.save(os.path.join(out_dir,"data_x.npy"),x)
          # np.save(os.path.join(out_dir,"data_y.npy"),y)
          del sentences
    else:
          print('data found...')
          x= np.load(os.path.join(out_dir,"data_x.npy"))
          y= np.load(os.path.join(out_dir,"data_y.npy"))
    print("x.shape = {}".format(x.shape))
    print("y.shape = {}".format(y.shape))

    # Save params
    if not os.path.exists(os.path.join(out_dir,"training_params.pickle")):
        training_params_file = os.path.join(out_dir, 'training_params.pickle')
        params = {'num_labels' : FLAGS.num_labels, 'max_document_length' : max_document_length}
        data_helper.saveDict(params, training_params_file)

    # Shuffle data randomly
    # np.random.seed(10)
    # shuffle_indices = np.random.permutation(np.arange(len(y)))
    # x_shuffled = x[shuffle_indices]
    # y_shuffled = y[shuffle_indices]
    # del x,y

    # x_train, x_test, y_train, y_test = train_test_split(x_shuffled, y_shuffled, test_size=0.2, random_state=42)  # split into training and testing set 80/20 ratio
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)  # split into training and testing set 80/20 ratio
    del x,y
    return x_train, x_test, y_train, y_test
```

è¯»å–æ•°æ®ï¼š

```python
    allurlscsv = pd.read_csv(path, ',', error_bad_lines=False)  # reading file
    allurlsdata = pd.DataFrame(allurlscsv)  # converting to a dataframe
    allurlsdata = np.array(allurlsdata)  # converting it into an array
    random.shuffle(allurlsdata)
```

![6fig1](/Malicious_URL_9/6fig1.png)

æŠŠæ ‡ç­¾æ›¿æ¢æˆæ•°å­—ï¼š

```python
    for i in range(0,len(y)):
        if y[i] =='bad':
            y[i]=0
        else:
            y[i]=1
```

ä½†æ˜¯å‡½æ•°to_categoricalå¥½åƒæœ‰ç‚¹é—®é¢˜ï¼Œæ ‡ç­¾è¯»å–çš„éƒ½æ˜¯1ã€‚ä½†æ˜¯å®Œæ•´æµ‹è¯•çš„æ—¶å€™å¥½åƒåˆæ­£å¸¸äº†ï½



ä¸‹é¢padding_sentenceså‡½æ•°ï¼Œç”¨äºå¡«å……æˆ–è€…åˆ‡é™¤ä¸€å®šçš„é•¿åº¦ã€‚

ä¸‹é¢çœ‹æ¨¡å‹æ˜¯å¦å­˜åœ¨ã€‚å¦‚æœä¸å­˜åœ¨ï¼šè°ƒç”¨embedding_sentencesï¼ŒåµŒå…¥å¥å­ã€‚

```python
if not os.path.exists(os.path.join(out_dir,"trained_word2vec.model")):
  x= np.array(word2vec_helpers.embedding_sentences(sentences, embedding_size = FLAGS.embedding_dim, file_to_save = os.path.join(out_dir, 'trained_word2vec.model')))
```

å¦‚æœå­˜åœ¨ï¼Œä¹Ÿæ˜¯è°ƒç”¨embedding_sentencesï¼Œç•¥å¾®æœ‰äº›ä¸åŒã€‚



å¦‚æœæœ‰data_x.npy,ç›´æ¥è½½å…¥æ•°æ®ã€‚



ä¸‹é¢ä¿å­˜å‚æ•°ï¼Œå¯ä»¥ç½®ä¹±ä¸€ä¸‹ï¼Œä½†æ˜¯æ–‡ä¸­åæ¥åˆ å»äº†ï½ç„¶ååˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚



ç„¶ååˆ¤æ–­ï¼Œå¦‚æœreplicas == Trueï¼Œæ‰§è¡Œä¸‹é¢çš„æ“ä½œã€‚

ä¸»è¦å°±æ˜¯é¢„å¤„ç†å’ŒCNNã€‚CNNå…·ä½“çš„ç»“æ„æ¯”è¾ƒå¤æ‚ï¼Œç°åœ¨å°±ä¸çœ‹å•¦ï½ï½

---

çœ‹è¯„ä»·çš„ä»£ç 

å‰é¢çš„ä¸»è¦å°±æ˜¯åŠ è½½å‚æ•°ï¼Œç„¶åè¿˜æ˜¯è¯»å–æ•°æ®ï¼Œè¯åµŒå…¥ã€‚åé¢çš„å‡†ç¡®åº¦æ˜¯è‡ªå·±æ‰‹åŠ¨è¾“å…¥çš„ï½

ä»£ç çœ‹å®Œäº†ï¼Œæ¯”è‡ªå·±æƒ³è±¡çš„è¦ç®€å•ä¸€äº›ã€‚

ä¸‹é¢è‡ªå·±å†™ä¸€ä¸ªword2vec + ä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„ç®—æ³•ï½

---



å•Šå•Šå•Šå•Šå•Šï¼åŸæœ¬word2vecåµŒå…¥ä¹‹åç»´åº¦è¿˜æ˜¯æœ‰ä¸€ç‚¹ç‚¹çš„é—®é¢˜ï¼Œä½¿ç”¨è¯­å¥`X = X.reshape(X.shape[0],-1)`ï¼Œå°†ä¸‰ç»´çš„è½¬ä¸ºäºŒç»´ã€‚



æ­¤æ—¶å‚æ•°è®¾ç½®ï¼šmax_sequence_length = 10ï¼Œembedding_dim = 12

ä½¿ç”¨å¾ˆå°‘çš„å‡ ç™¾æ¡æ•°æ®è¿›è¡Œæµ‹è¯•ï½æ¨¡å‹ä½¿ç”¨LRï¼Œç»“æœå¦‚ä¸‹

![6fig2](/Malicious_URL_9/6fig2.png)

å¤šç”¨ä¸€äº›æ•°æ®ï¼

LRï¼Œç»“æœå¦‚ä¸‹ï¼š

![6fig3](/Malicious_URL_9/6fig3.png)



å¦‚æœä½¿ç”¨å†³ç­–æ ‘ï¼Œç»“æœå¦‚ä¸‹ï¼š

![6fig4](/Malicious_URL_9/6fig4.png)

å•Šå•Šå•Š å•Šå•Šå•Š å•Šç»“æœè¿™ä¹ˆå¥½ï¼å†³ç­–æ ‘æˆ‘çˆ±ä½ ï¼



å†ä½¿ç”¨éšæœºæ£®æ—ï¼Œå—·å—·å•Šæ€ä¹ˆä¼šè¿™ä¹ˆå¥½ï¼Ÿæˆ‘å¼€å§‹æ€€ç–‘è‡ªå·±äº†

![6fig5](/Malicious_URL_9/6fig5.png)



å˜æ¢ä¸€ä¸‹å‚æ•°ï¼šmax_sequence_length = 15ï¼Œembedding_dim = 12

LRï¼šç›´æ¥killedäº†ï¼Œå¯èƒ½æ˜¯å‚æ•°è®¾ç½®æœ‰ç‚¹å¤§äº†ã€‚



å†³ç­–æ ‘ï¼šç•¥ç•¥æœ‰äº›ä¸‹é™ï½

![6fig6](/Malicious_URL_9/6fig6.png)

éšæœºæ£®æ—ï¼Œå‡†ç¡®åº¦ç•¥æœ‰æå‡ï½

![6fig7](/Malicious_URL_9/6fig7.png)



å†å˜æ¢ä¸€ä¸‹å‚æ•°ï¼šmax_sequence_length = 12ï¼Œembedding_dim = 12

LRï¼šç¨å¾®æœ‰ä¸€ç‚¹ç‚¹æå‡ï½

![6fig8](/Malicious_URL_9/6fig8.png)



å†³ç­–æ ‘ï¼šæœ‰ä¸€ç‚¹ç‚¹ç‚¹å°æå‡

![6fig9](/Malicious_URL_9/6fig9.png)



éšæœºæ£®æ—ï¼š

![6fig10](/Malicious_URL_9/6fig10.png)

çŸ©é˜µå˜æ¢é‚£é‡Œå°è¯•ä¸€ä¸‹ ä¸€ä¸ªurlçš„çŸ¢é‡å˜æˆä¸€ç»´

X = X.reshape(len(X),max_sequence_length*embedding_dim)

å…¶å®æ²¡å¤ªå¤§çš„å·®åˆ«å•¦ï½éšæœºæ£®æ—å¦‚ä¸‹ï¼š

![6fig11](/Malicious_URL_9/6fig11.png)



ä½¿ç”¨Doc2vev

å¾—åˆ°é”™è¯¯ï¼š'list' object has no attribute 'words'ï¼Œè¿™æ˜¯å› ä¸ºwordåé¢éœ€è¦åŠ ä¸Šæ ‡ç­¾ï¼Œæ ‡å·ã€‚

å‚è€ƒ https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/blob/master/NLP/Doc2vec/Doc2vec.py

ä¸å¤ªæ¸…æ¥šæ ‡ç­¾é‚£è¾¹æ˜¯æ€ä¹ˆå¼„çš„ã€‚

å…ˆç”¨ tagged_data = [TaggedDocument(words=_d, tags=[str(y[i])]) for i, _d in enumerate(sentences)]ï¼Œæ ‡ç­¾æ˜¯yçš„æ ‡ç­¾

ä½¿ç”¨éšæœºæ£®æ—ï¼Œå•Šè¿™ï¼Œæ¨¡å‹çš„å‡†ç¡®åº¦æœ‰ç‚¹è¿‡é«˜

![6fig12](/Malicious_URL_9/6fig12.png)



å¦‚æœæ ‡ç­¾ä½¿ç”¨tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) **for** i, _d **in** enumerate(sentences)]

æ•°æ®å¥½çš„æœ‰ç‚¹å¯æ€•ï½

éšæœºæ£®æ—

![6fig13](/Malicious_URL_9/6fig13.png)



å†³ç­–æ ‘

![6fig14](/Malicious_URL_9/6fig14.png)



å¦‚æœä½¿ç”¨é€»è¾‘å›å½’ï¼Œæ•ˆæœè¿˜æ˜¯æœ‰ä¸€ä¸ä¸çš„å·®ï½

![6fig15](/Malicious_URL_9/6fig15.png)



ä¸‹é¢æ˜¯ä¹‹å‰å‚è€ƒè¿‡çš„dord2vecï¼Œé‡ç‚¹çœ‹äººå®¶çš„æ ‡ç­¾çš„éƒ¨åˆ†



æµ‹è¯•ä»£ç å¦‚ä¸‹ï¼š

```python
#å…ˆå®‰è£…
pip install gensim

#ipythonä¸­å¦‚ä¸‹æ“ä½œ
import sys
import logging
import os
import gensim

# å¼•å…¥doc2vec
from gensim.models import Doc2Vec
curPath = os.path.abspath(os.path.dirname('10.csv'))
rootPath = os.path.split(curPath)[0]
sys.path.append(rootPath)
#from utilties import ko_title2words
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# åŠ è½½æ•°æ®
documents = []
# ä½¿ç”¨countå½“åšæ¯ä¸ªå¥å­çš„â€œæ ‡ç­¾â€ï¼Œæ ‡ç­¾å’Œæ¯ä¸ªå¥å­æ˜¯ä¸€ä¸€å¯¹åº”çš„
count = 0
with open('../data/titles/ko.video.corpus','r') as f:
    for line in f:
        title = unicode(line, 'utf-8')
        # åˆ‡è¯ï¼Œè¿”å›çš„ç»“æœæ˜¯åˆ—è¡¨ç±»å‹
        words = ko_title2words(title)
        # è¿™é‡Œdocumentsé‡Œçš„æ¯ä¸ªå…ƒç´ æ˜¯äºŒå…ƒç»„ï¼Œå…·ä½“å¯ä»¥æŸ¥çœ‹å‡½æ•°æ–‡æ¡£
        documents.append(gensim.models.doc2vec.TaggedDocument(words, [str(count)]))
        count += 1
        if count % 10000 == 0:
            logging.info('{} has loaded...'.format(count))

# æ¨¡å‹è®­ç»ƒ
model = Doc2Vec(documents, dm=1, size=100, window=8, min_count=5, workers=4)
# ä¿å­˜æ¨¡å‹
model.save('models/ko_d2v.model')
curPath
```



å…ˆçœ‹æ²¡æœ‰é¢„è®­ç»ƒçš„ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
import pandas as pd
import gensim.models as g
import logging

data = pd.read_csv("10data.csv")
y = data["label"]
url_list = data["url"]

vector_size = 300
window_size = 15
min_count = 1
sampling_threshold = 1e-5
negative_size = 5
train_epoch = 100
dm = 0 #0 = dbow; 1 = dmpv
worker_count = 1 #number of parallel processes
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

docs = g.doc2vec.TaggedLineDocument(url_list)
trans_vector = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, iter=train_epoch)
```

å‡ºç°å¦‚ä¸‹æŠ¥é”™

![6fig16](/Malicious_URL_9/6fig16.png)

å› ä¸ºTaggedLineDocumentçš„å‚æ•°éœ€è¦æ—¶æ–‡ä»¶çš„åœ°å€ã€‚ 

æ­¤å¤„éœ€è¦å°†urlè¿›è¡Œutf-8ç¼–ç ï¼Œç„¶ååˆåœ¨ä¸€èµ·ã€‚

```python
with open('../data/titles/ko.video.corpus','r') as f:
    for line in f:
        title = unicode(line, 'utf-8')
        # åˆ‡è¯ï¼Œè¿”å›çš„ç»“æœæ˜¯åˆ—è¡¨ç±»å‹
        words = ko_title2words(title)
        # è¿™é‡Œdocumentsé‡Œçš„æ¯ä¸ªå…ƒç´ æ˜¯äºŒå…ƒç»„ï¼Œå…·ä½“å¯ä»¥æŸ¥çœ‹å‡½æ•°æ–‡æ¡£
        documents.append(gensim.models.doc2vec.TaggedDocument(words, [str(count)]))
        count += 1
        if count % 10000 == 0:
            logging.info('{} has loaded...'.format(count))

# æ¨¡å‹è®­ç»ƒ
model = Doc2Vec(documents, dm=1, size=100, window=8, min_count=5, workers=4)
```



åˆ°ç°åœ¨ä¹Ÿè¿˜æ˜¯æ²¡æœ‰å®ç°...



å…ˆå®ç°ä¸€ä¸ªç®€å•çš„ï½å…³äºæ–°é—»çš„

åœè¯è¡¨å‚è€ƒ https://github.com/goto456/stopwords ä½¿ç”¨å››å·å¤§å­¦çš„ã€‚

ä»£ç å®ç°å¦‚ä¸‹

```python
from gensim import models,corpora,similarities
import jieba.posseg as pseg
from gensim.models.doc2vec import TaggedDocument,Doc2Vec
import os

def a_sub_b(a,b):
    ret = []
    for el in a:
        if el not in b:
            ret.append(el)
    return ret
stop = [line.strip().encode().decode('utf-8') for line in open('stopwords.txt').readlines() ]
```



stopæ˜¯ä¸€ä¸ªlistï¼Œå†…å®¹å¦‚ä¸‹ï¼š

![6fig17](/Malicious_URL_9/6fig17.png)

ç„¶åè¿˜æ˜¯æœ‰å¾ˆå¤šé”™è¯¯å•Šå•Šå•Šå•Šå•Šï¼Œä¸»è¦æ˜¯æ ¼å¼é—®é¢˜ï¼Œä¸æ”¹äº†æ°”æ­»äº†ï¼



è‡ªå·±ä¹Ÿåšé’“é±¼æ£€æµ‹ï¼ï¼

å†æµ‹è¯•ä¸€ä¸‹æ–°çš„æ•°æ®é›†ï¼ŒUNCå¤§å­¦2016å¹´çš„URLï¼Œçœ‹çœ‹è‡ªå·±çš„æ•ˆæœæ˜¯ä¸æ˜¯çœŸçš„é‚£ä¹ˆå¥½

æ˜¯çš„ï¼

æˆ‘ä»–å¦ˆçš„éƒ½æœ‰ç‚¹å®³æ€•ï¼Œåˆ°åº•æ˜¯ä¸æ˜¯è¿‡æ‹Ÿåˆäº†å•Šå“ˆå“ˆå“ˆå“ˆï¼Œè¦æ˜¯éƒ½è¯é‚£æˆ‘çœŸä»–å¦ˆå®Œè›‹äº†ã€‚



æœ€åå†³å®šä½¿ç”¨https://www.kaggle.com/siddharthkumar25/malicious-and-benign-urls/tasksçš„æ•°æ®é›†è¿›è¡Œæµ‹è¯•





æ”¶è·ï¼š

1ã€æ–‡ä¸­å¯ä»¥åŠ å…¥å¦‚ä¸‹è¯­å¥ï¼š

ä¸€èˆ¬é’“é±¼çš„é“¾æ¥ä¼šåœ¨åŸŸåå’Œä¸»æœºåä¹‹é—´ä½œæ–‡ç« ï¼Œè¿›è¡Œä¸€äº›åŸŸåæ··æ·†çš„æ¶æ„è¡Œä¸º

è€Œæ¶æ„ç”¨æˆ·è¯·æ±‚ä¼šä»è¯·æ±‚å‚æ•°ä½œæ–‡ç« ï¼Œæ¯”å¦‚è¿›è¡Œæ¶æ„SQLæ³¨å…¥



2ã€çªå‘å¥‡æƒ³ï¼šèƒ½ä¸èƒ½æ„å»ºä¸¤ä¸ªåº“ï¼Œä¸€ä¸ªè‰¯æ€§ä¸€ä¸ªæ¶æ„ï¼Œæœ€åçœ‹æŸ¥è¯¢çš„URLå’Œå“ªä¸ªçš„ç›¸ä¼¼åº¦æ›´é«˜ï¼Ÿ

ç»“è®ºï¼šè¿™ä¸ªå’Œæœ‰ç›‘ç£å­¦ä¹ å…¶å®å°±æ˜¯ä¸€ä¸ªæ€æƒ³ã€‚



3ã€å„ç§æ–¹æ³•æ¯”è¾ƒ

![6fig18](/Malicious_URL_9/6fig18.png)

![6fig19](/Malicious_URL_9/6fig19.png)



ä»£ç å®ç°è¿‡ç¨‹å‚è€ƒ https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/blob/master/NLP/Doc2vec/Doc2vec.py

